---
title: 月之森点子系列：HMM隐马尔科夫模型（Hidden Markov Model）
mathjax: true
categories: 随便写写
tags:
  - NLP
  - 期末复习
  - 隐马尔科夫模型
  - 月之森点子集
abbrlink: c59edc3f
date: 2024-11-16 14:25:47
---
[隐马尔可夫模型](https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B)参考链接见这里

## 月之森点子王最近的困扰

<center>
<img src="/pics/sakikiki.png" width="60%">
</center>

Sakiko最近总是不回复Soyo的消息，Soyo表示很担心Sakiko的安全~~（存疑）~~，常常夜不能寐。但是最近在从月之森同校的Mutsumi同学提供的消息来看，祥子最近去上海旅游了三天，去了三个地方，Sakiko第一天去三个地方的概率都是$\frac{1}{3}$，之后按照聊天记录里的概率进行转移，并且从Mutsumi同学口中得知，Sakiko第一天没有购物，第二第三天都进行了购物（~~最不符合经济现状的一集~~）；

<center>
<img src="/pics/1731764039925.png" width="30%">
</center>

看到Sakiko聊天记录的Soyo~~（蓑鱿~~即刻激发出了惊人的数学天赋，点子计从心来，她想起了她小学就学过的隐马尔可夫模型（HMM 非常适合解决~~追踪~~Sakiko三天的行踪🤩🤩🤩

Soyo不亏是月之森点子王，HMM确实很适合研究这种马尔科夫过程的隐状态预测。

## 什么是隐马尔科夫模型

隐马尔可夫模型（英语：Hidden Markov Model；缩写：HMM），或称作隐性马尔可夫模型，是统计模型，用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析。**在隐马尔可夫模型中，状态并不是直接可见的，但受状态影响的某些变量则是可见的。**

就像下图所展示的，隐马尔科夫模型的定义：

设$X_n,Y_n$为离散~~其实连续也行~~时间随机过程，$n\geq1$，则$(X_n,Y_N)$为隐马尔科夫模型的条件是：

- $X_n$是马尔科夫过程，其行为无法直接观测
- $P(Y_n|X_1=x_1,\dots X_n=x_n)=P(Y_n|X_n=x_n)$，人话就是$Y$当前的状态只取决于$X$当前的状态

<center>
<img src="/pics/1731740274318.png" width="60%">
</center>

上图展示了HMM的状态迁移，由图中所展示的$X(t)$仅和$X(t-1)$有关，而$X(t-1)$又和$X(t-2)$有关。每个$Y(t)$仅仅由$X(t)$决定。

假设我们观察到的结果为$Y$

$$
Y=y(0),y(1)\dots ,y(L-1)
$$

观察不到的隐藏状态为$X$

$$
X=x(0),x(1)\dots,x(L-1)
$$

长度都是$L$，那么马尔科夫链的概率可以表述为

$$
X=\sum_{X}P(Y|X)P(X)
$$

## 用HMM来做什么？

HMM有三个典型的问题：

- 预测（filter）：已知模型一系列特定输出序列求最后一个时刻的隐状态概率分布，也就是求：

  $$
  P(x(t)|y(1),y(2)\dots ,y(t))
  $$
- 平滑处理（smoothing）：已知模型某一特定显状态输出序列，求中间某时刻隐状态的概率分布：

  $$
  P(x(k)|y(1),y(2)\dots ,y(t))\quad where\quad k\in (1,t)
  $$
- 解码（most likely explaination）：已知模型参数，寻找最可能的能产生某一特定输出序列的隐状态序列，即求：

  $$
  P([x(1)\dots x(t)]|[y(1)\dots y(t)])
  $$

## 破解Sakiko行踪问题！

在祥子的行踪问题中，祥子每天所在的位置就是隐状态，因为Soyo没有办法给Sakiko安装GPS定位，但是祥子购物与否便是可以通过睦头人口中得知的可观察状态：

$$
A=\left[
    \begin{matrix}
    0.80&0.05&0.15\\
    0.10&0.60&0.30\\
    0.10&0.40&0.50\\
    \end{matrix}
\right]
$$

由这个隐状态决定的显性概率我们也可以用一个矩阵$B$表示：

$$
B=\left[
\begin{matrix}
0.1&0.8&0.3\\
0.9&0.2&0.7\\
\end{matrix}
\right]
$$

有了这样的两个矩阵$A,B$。Soyo就成功建立起来了祥子的行踪追踪模型。

> 首先是预测filter

预测是HMM最简单的一招，比如我们需要预测祥子接下来连续10天购物的概率如何预测？

$$
P(V^{1:T})=P(V^1=2,V^2=2\dots,V^{10}=2|A,B,S_0)
$$

这$S_0$表示的是最开始Sakiko到三地概率相等的初始状态。这个概率计算的难点主要是在于虽然已知HMM的转换概率，但是我们无法确认隐状态的序列。直觉上我们只需要遍历所有可能的状态即可，例如我们假设一个隐状态前5次祥子到达的都是欢乐谷，后五次都是在外滩：

$$
w_r|_{given}=\{w_r^1=1,w_r^2=1\dots w_r^{10}=3 \}
$$

于是我们可以计算这种隐状态序列下投出10个6的概率：

$$
P(V^{1:T}|w_r)=\prod_{i=1}^TP(v(t)|w_r{(t)})=\prod_{i=1}^Tb_{w(t)v(t)}
$$

然后把所有可能的概率累加即可：

$$
P(V^{1:T})=\sum P(V^{1:T}|w_r)
$$

嗯嗯，至此问题貌似已经解决了。换一种用递推思路解决的写法：

$$
\begin{align*}
P(V^{1:T})&=\sum_{w{(T)}}P(V^{1:T},w(T))\\
&=\sum_{w(T)}P(V^{1:T-1},V^T,w(T))\\
&=\sum_{w(T)}P(V^T|V^{1:T-1},w(T))\cdot P(V^{1:T-1},w(T))\\
&=\sum_{w(T)|w(T-1)}(P(V^T|w(T))\cdot\sum_{w(T-1)}P(w(T)|w(T-1))\cdot P(V^{1:T-1},w(T-1)) )\\
\end{align*}
$$

有了这个递推关系，我们便能从$T=1$开始一直推导出$P(V^{1:T})$的答案。

> 解码过程：Find the Most Likely Explaination

通俗来说：解码的过程就是在给出一串序列的情况下和已知HMM模型的情况下，找到最可能的隐性状态序列。

一个~~不恰当的~~比喻就是Sakiko接下来10天都有消费记录，那么Soyo需要估计祥子这十天去了哪里，这就是一个HMM的解码问题！

形式化的描述为下面的式子：

$$
\mathcal{MAX}_{w({1:T})}P(w(1:T),V^{1:T})
$$

根据逻辑链条：

$$
\begin{align*}
P(w(1:T),V^{1:T})&=P(V^T|w(T))\cdot P(w(T)|w(T-1))\cdot P(V^{1:T-1},w(1:T-1))
\end{align*}
$$

我们从$T=1$开始前向推导便可以得到这个概率的值；相对的我们可以使用一种后向追踪法求解出能使这个$P(w(1:T),V^{1:T})$概率最大的隐状态链条$w(1:T)$，最典型的实现是一种基于动态规划的维特比算法（$\mathcal{Viterbi\quad Algorithm}$）【更多详细的知识补充见附录部分】

为了介绍维特比算法，我们需要将这个HMM的解码问题转化为一个最佳路径选择的问题：

$$
state\_gragh(HMM)=\left[
\begin{matrix}
1&2&\dots&T\\
\dots&\dots&\dots&\dots\\
\dots&\dots&\dots&\dots\\
q^1_{N}&q^2_{N}&\dots&q^T_{N}\\
\end{matrix}
\right]
$$

我们将原本HMM的状态转移过程使用上述矩阵描述的从左向右选择的一幅图来看待，为此我们还需要定义几个符号:

- $a_{i,j}$表示$A$矩阵的第$i$行第$j$列的值，表示从隐状态$i$切换到$j$的概率。
- $b_{j}(V^T)$表示第$j$个隐状态表现显状态为$V^T$的概率。
- $\delta_t(i)$表示第$t$个位置上的隐状态是$i$，其他位置概率任选的最大概率。

$$
\begin{align*}
\delta_{t}(i)&=max_{}P(V^1,V^2,\dots,V^i, w(1),w(2)\dots,w(t)=i)\\
\delta_{t+1}(j)&=\max_{1\leq i\leq N}(\delta_t(i)a_{i,j})b_j(V^{T+1})
\end{align*}
$$

- $\phi_{t+1}(j)$表示当前隐状态为$j$时上一个时刻最有可能的隐状态的值。

$$
\phi_{t+1}(j)=argmax_{1\leq i<N}\delta_t(i)a_{i,j}
$$

给出上述定义之后，我们的维特比算法已经呼之欲出力！其实维特比算法就是一种动态规划的原理，我们要求的其实就是首先取$t=T$，然后目标是$argmax_{j\in w}\delta_{t}(j)$，为此我们需要利用上述的递推关系式，根据$t=T-1$的$\delta_t$来计算，一直回退到$t=0$，将一路上确定的下标拼接在一起即可解码出最大概率隐状态链条。

至此Sakiko的行踪已经被蓑鱿彻底掌握！

---

素食剪切线

---

### 月之森点子王 `蓑鱿`的一些反思

我们都知道月之森点子王是不会满足于已有的知识的，他开始反思总结这次的方法，这次的隐马尔可夫模型存在很多显然的缺点：

- 我们需要事先知道Sakiko的所有目的地，并且只适用于静态策略，也就是转移状态是一个固定矩阵
- 马尔科夫过程的限制，使得我们当前隐状态只由前一次的状态直接决定，这从根本上限制了HMM难以处理复杂任务（~~点子王Soyo当然知道RNN可以解决这种缺点,所以下一篇文章就决定讲讲RNN~~

假设下次木子米拒绝告诉Soyo祥子的目的地那么强如soyo也只能束手无策。这次姑且以蓑鱿的胜利告终。

---

<center>
<img src="/pics/1731777488892.png" width="55%">
</center>
