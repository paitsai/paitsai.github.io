[{"title":"LLM剪枝-SparseGPT方法","url":"/post/20461be3.html","content":"‘SparseGPT’ one-shot Pruning Strategy【论文地址】SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot\nSparseGPT简介SparseGPT是一种用于压缩Massive LLM的一次性（one-shot）、无须重训练（no need for retraining）的基于非结构化Pruning（剪枝）的模型压缩方法，发现了至少50%的稀疏性；\n\nSparseGPT提出的创新点何在？其实就是两点：ONE-SHOT &amp;&amp; NO RETRAINING;\n\n\n\n\n\n上图就是原来的模型减枝之后，我们仍然需要一个Sparse Retraining的过程来调整稀疏化之后的模型，SparseGPT提出的剪枝方法则是one-shot的，也就是无须后面retraining或者说调整的代价很小。\nSparseGPT的基本原理已有方法存在的问题：一般的模型减枝（Pruning）都包含两步——Mask Selection 和 weight restruction。\n假设我们某一层的模型权重记作$W\\mathcal{L}，输入记作\\mathcal{X}\\mathcal{L}，掩码矩阵记作M\\mathcal{L}，之后的权重变成了\\tilde{W\\mathcal{L}}$，那么我们的最优化目标就变成了：\n\nargmin_{M_\\mathcal{L},\\tilde{W_\\mathcal{L}}} \\|W_\\mathcal{L}\\mathcal{X}_\\mathcal{L}-(M_\\mathcal{L}\\bigodot \\tilde{W_\\mathcal{L}} \\mathcal{X}_\\mathcal{L})  \\|_2^2但是因为${M\\mathcal{L},\\tilde{W\\mathcal{L}}}$这两部分会同时影响到上述最优化的结果，也有证明这是一个NP-hard问题，在巨大的模型面前解决这个问题是不现实的，我们需要另找办法。一种有效的方法就是根据一些显式的法则（比如直接根据权重来筛选）来实现直接给定一个MASK。然后接着做权重重建即可。\n根据上述分析和推导，我们的权重重建过程可以化为一个最小二乘法的最优化问题，形式通解可以描述为：\n\n\\mathcal{W}^i_{\\mathcal{M}_i}|_{update}=(\\mathcal{X}_{\\mathcal{M}_i}{\\mathcal{X}_{\\mathcal{M}_i}}^T)^{-1}{\\mathcal{X}_{\\mathcal{M}_i}}(\\mathcal{W}_{\\mathcal{M}_i}{\\mathcal{X}_{\\mathcal{M}_i}})^T\\\\我们不妨定义海森矩阵：\n\nH_{\\mathcal{M_i}}=\\mathcal{X}_\\mathcal{\\mathcal{M}_i}{\\mathcal{X}_\\mathcal{\\mathcal{M}_i}}^T这里面的${\\mathcal{X}{\\mathcal{M}_i}}指的是经过掩码的第i行之后仍然存在的输入；\\mathcal{W}{\\mathcal{M}i}是对应的权重；\\mathcal{W}^i{\\mathcal{M}_i}$是第i行更新之后的权重。\n但是这样的方法仍然会存在很多问题：\n\n最重要的一点是：掩码每一行不同会导致不同的海森矩阵，导致计算量巨大。并且$(H{\\mathcal{M_i}})^{-1}\\neq(H)^{-1}{\\mathcal{M_i}}$，计算矩阵的逆也十分消耗计算资源，就像下图所展示的这样。\n\n\n\n\n\n\n！素食剪切线警告！\n\n\n基于OBS思想的权重重建\n一种等价的迭代视角\n\n作者借鉴了Optimal Brain Surgery【OBS】（相关介绍见附录部分）中调整剩余权重来减少Pruning所减去当前权重影响的思想来对现有的方法进行改进。但是相比较于原来OBS更改全局参数的策略，SparseGPT则是使用一种更加高效的方法，每次只更新未被Pruning掉的部分；\n\n\n\n\n上图就是 SparseGPT Pruning算法的可视化，每次对一列权重进行Pruning（白色的块是被减掉的），右侧深蓝色的块就会被参数更新来补偿修建错误。没被修改的块就不会对后续参数造成影响。\n工作亮点：\n\n1）海森矩阵的处理\n\n我们将输入的特征矩阵来进行编号，然后我们定义特征矩阵子集的简记方式：\n\nU_{j+1}=U_j-{j}; \\quad U_1=\\{1,2\\dots,d_{col}\\}也就是说$U1代表的是全集；U{j+1}是在U_j的基础上删除序号为j$的元素.于是有：\n\n(H_{U_j})^{-1}=((\\mathcal{X}\\mathcal{X}^T)_{U_j})^{-1}根据这篇论文的工作，我们选取依次少选取一行的优势就显示出来了，我们计算海森矩阵的逆可以根据上一步的逆很快的得到。\n设$B=(H{U_j})^{-1}，那么在\\mathcal{O}(d{col}^2)$的时间之内我们可以计算出来：\n\n(H_{U_{j+1}})^{-1}=(B-\\dfrac{1}{[B]_{1,1}}\\cdot B_{:,1}B_{1,:})_{2:,2:}相比于原来$\\mathcal{O}(d{col}^3)的复杂度来计算一个矩阵的逆，现在充分利用已有信息可以在\\mathcal{O}(d{col}^2)$得到答案；\n\n2）计算复杂度分析\n\n通过上述的分析我们可以看到整体的计算开销主要由三部分组成：\n\n（a）初始Hessian矩阵的计算$T1=\\mathcal{O}(n\\cdot d{col}^2)$，其中n是输入特征向量的数量；\n（b）计算初始Hessian矩阵的逆$T2=\\mathcal{O}(d{col}^3)$；\n（c）然后对每一行使用权重重建$T3=\\mathcal{O}(d{col}^2d_{row})$\n\n总结：总共的时间复杂度就是$\\mathcal{O}(d{col}^3+d{row}d{col}^2)对于系列的模型，可以简化为\\mathcal{O}(h{hidden}^3)$的复杂度。\n自适应掩码选择在此之前，我们主要集中于谈论权重重建的细节，都是基于一个固定的Pruning Mask来进行的权重重建。已有的Mask Selection方法可以参考基于幅度（magnitude）选取的方法；一个最直观地做法就是选取每一列值最小的的权重，这样可以直接构造出的稀疏性，但是这样对每一行来说是不平均的，特别是transformer这样的架构会有少量高敏感的权重。\n为了解决这一点所存在的问题，原文使用了一种叫做迭代阻塞（iterative blocking）的方法。相比于每次选取一列来做一个的pruning，文中每次选取来进行一个mask的选择，这样可以一定程度上避免这种pruning不均匀的现象；\n同时SparseGPT稀疏化方法同样可以适用于很多半结构化稀疏性，例如很有名的n:m细粒度结构化稀疏性，所谓N:M细粒度结构化稀疏即在M个连续的权重值中固定有N个非零值，其余元素均为零值，例如下图中N=2，M=4的N:M稀疏（2:4稀疏）。N:M稀疏的权重矩阵可以进行压缩存储，仅保留所有的非零元素，再辅以一个indices矩阵指示非零元素在原矩阵中的空间位置。从实验结果上来看，采用N:M细粒度结构化稀疏方案的模型在权重存储量和计算量大减的同时取得了比肩甚至超过稠密的原网络的推理精度。\n\n\n至此我们可以就更具给定的来确定，来完成Mask Selection的工作。\n整体的算法流程可以描述如下：\n\n\n\n\n实验验证部分\n硬件条件：单块80G A100显卡\n模型：BLOOM-176B 、OPT-175B系列模型（都是decoder-only的架构）\n衡量指标：Perplexity困惑度【见附录】\n测试数据集：raw-WiKiText2、PTB、C4\nSparsify时间：4h左右\n\n\nSparseGPT和幅度剪枝效果对比\n\n可以发现，SparseGPT算法能够提供更多的稀疏性：\n\n\n\n\n\n\n\n\n\n\n实验结果表明：相比于规模性对较小的视觉任务的模型，LLM在50%的sparsity的设定下，幅度减枝（magnitude pruning）效果下降更加明显。但是对于SparseGPT而言，这种趋势相对平缓许多，对于2.7B的模型，Perplexity的下降已经接近一个百分点，到175B的模型时，下降已经接近于0；\n\n\n\n\n另外对于模型不同部位稀疏化影响敏感性的研究：\nlater layers are more sensitive than earlier ones\n\n\n\n\n\nAppendix结构化剪枝VS非结构化剪枝技术常见的稀疏方式可分为结构化稀疏和非结构化稀疏。前者在某个特定维度（特征通道、卷积核等等）上对卷积、矩阵乘法做剪枝操作，然后生成一个更小的模型结构，这样可以复用已有的卷积、矩阵乘计算，无需特殊实现推理算子；后者以每一个参数为单元稀疏化，然而并不会改变参数矩阵的形状，只是变成了含有大量零值的稀疏矩阵，所以更依赖于推理库、硬件对于稀疏后矩阵运算的加速能力。\n半结构化剪枝是一种介于结构化剪枝和非结构化剪枝之间的剪枝方法，可以同时实现高精度和结构正则化。半结构化剪枝通常基于特定的模式进行剪枝，这些模式可以是任意的，但需要经过精心设计以减轻性能下降并实现特定的加速效果。半结构化剪枝可以被视为一种细粒度的结构化剪枝方法。\n参考阅读资料\nOBS：‘optimal brain surgery’ 的介绍：OBS这个名字很有意思，翻译过来就是最佳脑外科手术的意思；很贴合OBS对神经网络所做的事情：他将神经网络中一些不重要的权重、连接给切除之后，再对其他权重做调整来最小化减去神经的损失，OBS这个名字非常切合。\n我们考虑对神经网络的误差函数进行泰勒展开可以都得到：\n\n\\delta \\mathcal{E}=(\\dfrac{\\delta\\mathcal{E}}{\\delta \\mathcal{W}})^T\\cdot \\delta\\mathcal{W}+\\dfrac{1}{2}{\\delta\\mathcal{W}}^TH\\delta\\mathcal{W}+\\mathcal{O}(\\|\\delta\\mathcal{W}\\|^3)其中为Hessian矩阵，代表的模型当前的权重参数，代表训练误差。训练神经网络用任意的优化算法，该剪枝算法都是适用的。\n我们可以通过一些梯度下降优化算法来找到一个局部最小解，上述公式第一项就等于0，再忽略三阶无穷小，可以得到：\n\n\\delta\\mathcal{E}=\\dfrac{1}{2}\\delta\\mathcal{W}^T H\\delta\\mathcal{W}这个时候我们开始对较小的权重进行减枝，比如我需要剪切权重中的第个元素那么我可以这样描述：\n\ne_q^T\\delta \\mathcal{W}+\\mathcal{W}_q=0其中为单位向量，只有在第项为1，其余项皆为0；这意味着什么？这意味着进行下一步更新的时候将被直接置于0！相当于完成了剪切；\n由上述的推导，我们现在将问题转化为了一个带约束条件的最优化问题，写出拉格朗日方程：\n\nL=\\dfrac{1}{2}\\delta\\mathcal{W}^T H\\delta\\mathcal{W}+\\lambda (e_q^T\\delta \\mathcal{W}+\\mathcal{W}_q)解这个拉格朗日方程我们可以得到：\n\n\\delta\\mathcal{W}=-\\frac{\\mathcal{W}_q}{[H^{-1}]_{qq}}\\cdot H_{:,q}^{-1};\\quad \\delta L_q=\\frac{\\mathcal{W}_q^2}{2[H^{-1}]}_{qq}Perplexity in LLM困惑度在LLM中的定义是：\n\nPP(X)=exp(-\\frac{1}{t}\\sum_{i=1}^t logp(x_i|x_{j","categories":["学术"],"tags":["大模型压缩","剪枝","Pruning"]},{"title":"GEAR:KV cache压缩框架精读","url":"/post/9b786805.html","content":"GEAR: 一種高效的近乎無損推理的LLM的 KV cache 壓縮策略論文地址在這裡GEAR: An Effective KV Cache Compression Recipe  for Near-Lossless Generative Inference of LLM\n研究背景原文中作者總結了現在階段為了解決GPU Memory問題的流行的幾種方法:\n(a)使用offload技術,通過將GPU的內存消耗轉移到CPU使用的內存or NVMe的存儲空間上.這種方式對總線帶寬(bandwidth)需求極大\n(b)緊接著提出來的是tokens dropping技術(比如我們上一篇文章StreamLLM也屬於這一類),這類方法屬於是利用注意力分佈的稀疏性,將註意力分數低下的tokens捨棄達到降低顯存消耗的目的.\n(c)另一種經常使用的量化技術(quantization),通過將全精度的數據轉化為半精度的數據進行存儲來降低顯存消耗.\n上述的三種方法:(a)會依賴於總線的帶寬來達到GPU和CPU之間高速的數據傳送.(b),(c)兩種方式雖然在絕大部分任務中都能高效的降低顯存佔用,並且對推理效果的損失也極低;但是在復雜的生成式任務中(比如涉及邏輯推理,解決數理問題)這兩種方法都存在普遍且明顯的效能損失.\n在較為簡單的任務中,模型只需要產生少數tokens從少數特定的上下文中就可以完整正確的自回歸任務.然而,在複雜的任務中,通常需要模型依據大量相關的上下文tokens產生更長更多的tokens;然而自回歸的docode過程中每一步得會累積誤差;\n\n  \n\n\n積累的誤差如上圖所示.在這個背景下,為了改善這種情形原文作者提出了GEAR用來減少KV cache量化的估計誤差.\n深入分析GEAR細節前置知识(i) 基础量化方法\n比如说我们有一个tensor  作为输入, 想要将这样一个输入的tensor做一个带宽(bandwidth)为b的量化操作,可以描述如下:\n\nQuant_{b,g}^{per-token}=\\dfrac{X_{\\mathcal{G}_i}-min(X_{\\mathcal{G}})}{max(X_{\\mathcal{G}})-min(X_{\\mathcal{G}})}\\times (2^b-1)其中  是指一个量化分组的size; 通常  取得越小量化效果越好,但是与此同时g取得越多需要保存的缩放因子也越多会导致内存消耗变大.\n(ii) MHA 多头注意力机制\n关于多头注意力的分析前面的文章以及分析过不少了,这里仅给出形式化的公式:\n\n\\begin{align}\nMHA(X)&=concat(H^{(1)},H^{(2)},...,H^{(h)})\\cdot W_O\\\\\nH_i&=Softmax(\\dfrac{Q^{(i)}K^{(i)T}}{\\sqrt{d_H}})\\cdot V^{(i)}\\\\\n\\end{align}GEAR的總體框架GEAR的整體思路其實很簡單,主要可以描述為以下三步:\n(i) 首先對KV cache採用一個常規的量化方法(比如將全進度float16的kv值全部轉儲為int2的類型),但是這必然會導致精度的大幅降低.\n(ii) 然后引入一个低秩矩阵来高效的估计量化之后的残差;\n(iii) 最后再引入一个稀疏矩阵来补全一些异常值导致的极个别的大额误差;\n省流版: 在原来粗暴量化的基础上,整体绝大部分的误差是通过引入一个低秩矩阵来解决的,而一些异常值是通过一个稀疏矩阵来恢复的;\n\n符号规定:\n\n量化之后的kv cache矩阵为 ; 上文提及的低秩矩阵记作 ; 用于捕捉补偿少部分异常值的稀疏矩阵记作 ;\n\n基本策略:\n\n给定一个待处理的tensor  ,我们的策略就是上文提及的三种量化策略之后得到的三部分矩阵, 然后最小化  和上述三部分的距离;所以实际上, 这个任务可以描述为:\n\nminimize\\|\\mathcal{X}-\\hat{\\mathcal{D}}-\\mathcal{L}-\\mathcal{S}\\|(i) 我们都知道过大或者过小的异常值会对量化过程的精度造成极大的影响,所以最佳的策略是在量化之前先进行一次异常值提取, 具体而言:\n\n\\begin{align}\n\\mathcal{S}&=Filter_S(\\mathcal{X})\\\\\nFilter_S(\\mathcal{X})&=\\left\\{\n\\begin{array}{l}\n\\mathcal{X_{ij}},s.t. \\mathcal{X}=K_t and \\mathcal{X}_{ij}\\text{in top/buttom} \\frac{s}{2}\\% \\text{of the j-th channel}{\\mathcal{X}_{*j}} \\\\\n\\mathcal{X_{ij}}, s.t. \\mathcal{X}=V_t and \\mathcal{X}_{ij}\\text{in top/buttom} \\frac{s}{2}\\% \\text{of the i-th token} {\\mathcal{X}_{i*}}\\\\\n0,  s.t.else.\n\\end{array}\n\\right.\\\\\n\\end{align}在异常值提取完成之后,再接着进行量化处理:\n\n\\hat{\\mathcal{D}}=Quant_{b}^{\\text{Selected Scheme}}(\\mathcal{X}-\\mathcal{S}).这样的思路其实在之前早已被应用于LLM的权重量化上, 但是相比于对于权重(weight)量化而言, kv cahce拥有更多的异常值(outliers),使得异常值提取的重要性更大了;\n(ii) 提取完成异常值之后再进行低秩矩阵误差估计;\n根据上文的说法, 我们定义的低秩残差为 ;\n然后我们将上述低秩残差分作  个多头子矩阵, 其中  是第h个头的残差矩阵: \n设 $\\mathcal{R}h的奇异值分解形式如\\sum{i=1}^{k}\\sigma_i\\mu_i m_i^T其中\\sigma_1&gt;\\cdots&gt;\\sigma_k是\\mathcal{R}_h的奇异值\\mu_i和m_i$ 为对应的特征向量;\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n附錄(a)理解低秩矩陣和稀疏矩陣\n低秩矩陣和稀疏矩陣 的相同点在于都表明矩阵的信息冗余比较大.具体来说，稀疏意味着有很多零，即可以压缩；低秩意味着矩阵有很多行（列）是线性相关的.low rank matrix和稀疏矩陣各有各的用途.\n\n補充知識點1 稀疏表示!\n\n假設有維的n個輸入,m非常之大,我們想要一勞永逸,不想要存儲所有的m個向量,於是我們想要學習到一種表示:\n\n\\begin{align*}\n&=argmin_{D,E}\\|X-DZ \\| \\\\\ns.t.\\|Z_i\\|_{0} &\\leq \\Delta \\\\\n\\end{align*}這裡的我們稱做字典,是輸入基於字典的稀疏表示法;是一個較小的值,第二個式子能夠保證矩陣是一個稀疏矩陣.尋求上式的最優解是一個NP-Hard問題,我們可以使用一些算法來快速地得到一些次優解.\n針對上述問題,我們可以先選取一組稀疏表示的初始解:,其中是滿足上述第二個不等式約束的.\n然後的優化目標變成了:\n\nminimize \\quad \\|X-DZ\\|+\\lambda \\sum_{i} \\|{X_0}_i\\|這裡存在兩個優化變量,一般的策略就是固定其中一個變量然後動態更新另外一個變量,下面以更新字典為例子.\n假設我們固定了稀疏表示,在此基礎上來逐列更新字典的第k列:\n\n\\begin{align*}\n\\|X-DZ\\|&=\\|X-\\sum_{j=1}^{K}d_j\\cdot z_j\\|\\\\\n&=\\|(X-\\sum_{i\\neq k}d_j\\cdot x_j)-d_k\\cdot z_k\\|\\\\\n&=\\|E_k-d_k\\cdot z_k\\|\n\\end{align*}上式中 $Ek=(X-\\sum{i\\neq k}dj\\cdot x_j)被定義為殘差此時最優化問題可以被描述為min{d_k}|E_k-d_k\\cdot z_k|$ ,這顯然是一個最小二乘問題,可以直接用最小二乘法就可以解決這個問題.\n但是這裡仍然需要註意的問題是,我們不能直接使用進行求解,因為不加限製的求解時不能保證稀疏性.我們需要選取出中不為0的部分再進行迭代更新.就像下圖所展示的一樣:\n\n  \n\n\n參考資料稀疏表示\n\n補充知識點2 低秩(low rank represent 低排名🐻‍❄️)表示!\n\n假設一個輸入信號由低秩矩陣和噪聲組成,即,为了還原低秩矩阵，求解如下最小化问题：\n\n=arg min(rank(R))+\\lambda\\|S\\|_{l=0},s.t.X=R+S;然而,矩陣rank的計算和L0范數通常是非凸的,考慮到這點我們通常使用矩陣的核范數 (矩陣奇異值的和) 和L1范數   對上式進行鬆弛處理,\n\n=arg min(\\|R\\|*)+\\lambda\\|S\\|_{l=1},s.t.X=R+S;從而得到一個凸優化問題.\n(b)稀疏子空間聚類(Sparse Subspace Clustering, SSC)\n稀疏子空間聚類問題(SCC)可以描述為:假設有一組高維數據點的集合,其中,是高維空間中的點(这些点分布在 K KK 个低维子空间上，每个子空间的维数远小于数据点的原始维度，即 d_k &lt;&lt; D ).對於這一組數據我們期望尋找一組稀疏向量,使得能夠被其他數據點的線性組合來逼近.\n和上面低秩表示一樣我們定義一個鬆弛化的SCC最小化函數\n\nminimize\\quad \\|x_i-X\\cdot z_i\\|_2+ \\lambda \\|z_i\\|_1上式意味著我們在整體數據集上為每一個數據點尋找一個盡可能稀疏的表述法則,從而將數據進行聚類.\n\n  \n\n\n\n\n\n","categories":["学术"],"tags":["大模型压缩"]},{"title":"SteamingLLM","url":"/post/17847744.html","content":"只会开新坑不会填旧坑的屑Mr.Xau🕊️🕊️🕊️ 又打算分享一篇基于KV Cache优化经典工作\n本人近期任务太多以后一定会填坑的（下次一定🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺🥺\n🤓🤓🤓🤓🤓 已填坑完成！ 🤓🤓🤓🤓🤓\nStreamLLM 基于注意力汇聚现象的KV Cache策略论文地址，Streaming这个词很有意思，~不是我们玩的steam~ ，它可以是喷射的意思，也可以是涓涓细流的意思；我觉得从这篇工作的内容来看，翻译为娟娟溪流可能更加合适一点。那么这个娟娟细流到底指的是LLM中的什么，但是正所谓铁打的衙门流水的官,KV Cache中有没有铁打不变的东西呢？且听后文分析。\n知识补充主要是KV Cache的介绍：什么是KV Cache，为什么只缓存KV？\n什么是KV Cache？回忆一下Transformer中的注意力机制，在经典的Transformer中我们有向量化的语料输入Tokens序列，如果batch size=1， 的形状是 ，其中是输入序列的长度 ~一句话单词数~ ， 是给每个单词的编码向量的维度 ;经过注意力编码之后有（暂时以一个头的注意力为例子）：\n\n\\begin{align*}\n\\mathcal{Q}&=W_Q\\cdot \\mathcal{X}\\\\\n\\mathcal{K}&=W_K\\cdot \\mathcal{X}\\\\\n\\mathcal{V}&=W_V\\cdot \\mathcal{X}\\\\\n\\end{align*}我们通过这个编码之后的矩阵，可以计算输出的注意力分数：\n\n\\mathcal{Att_s}=\\mathcal{QK}^T我们把展开写看看：\n\n\\begin{align*}\n\\mathcal{Att_s}&=\\mathcal{QK}^T\\\\\n&=\\begin{bmatrix}\nq_1  \\\\\nq_2  \\\\\n\\vdots \\\\\nq_l\n\\end{bmatrix}\\cdot [k_1,k_2,\\cdots k_l]\\\\\n&=\\begin{bmatrix}\nq_{1}\\cdot k_{1} & q_{1}\\cdot k_{2} & \\cdots & q_{1}\\cdot k_{l} \\\\\nq_{2}\\cdot k_{1} & q_{2}\\cdot k_{2} & \\cdots & q_{2}\\cdot k_{l} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nq_{l}\\cdot k_{1} & q_{l}\\cdot k_{2} & \\cdots & q_{l}\\cdot k_{l}\n\\end{bmatrix}\n\\end{align*}如果采用仅解码器的架构，由于掩码的存在，会有：\n\n\\begin{align*}\n\\mathcal{Att_s}=\\begin{bmatrix}\nq_{1}\\cdot k_{1} & 0 & \\cdots & 0 \\\\\nq_{2}\\cdot k_{1} & q_{2}\\cdot k_{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nq_{l}\\cdot k_{1} & q_{l}\\cdot k_{2} & \\cdots & q_{l}\\cdot k_{l}\n\\end{bmatrix}\n\\end{align*}我们都知道每次LLM会将上一次输出的一个token放在下次输入的最后一个，那么下一轮的注意力分数是：\n\n\\begin{align*}\n\\mathcal{Att_s|_{next}}=\\begin{bmatrix}\nq_{1}\\cdot k_{1} & 0 & \\cdots & 0 & 0 \\\\\nq_{2}\\cdot k_{1} & q_{2}\\cdot k_{2} & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots& \\vdots \\\\\nq_{l}\\cdot k_{1} & q_{l}\\cdot k_{2} & \\cdots & q_{l}\\cdot k_{l} & 0\\\\\nq_{l+1}\\cdot k_{1} & q_{l+1}\\cdot k_{2} & \\cdots & q_{l+1}\\cdot k_{l} & q_{l+1}\\cdot k_{l+1}\\\\\n\\end{bmatrix}\n\\end{align*}显然因为掩码的存在，注意力分数变成了一个下三角矩阵，因此当我们产生了新的tokens的时候，只需要计算最新tokens的q查询向量$q{l+1}和k{k+1}即可；这样的结构我们很自然就能想到，每次进行推理的时候将之前1\\to k的\\mathcal{K},\\mathcal{V}$都缓存起来，下次计算注意力分数的时候便无须再次重新计算了；\n从上面的公式的形状，我们也不难发现，完全没有必要缓存矩阵，因为每次下三角矩阵扩展的时候都只用到了这一列，所以，之前计算的对于后面的计算是没有作用的我们可以完全舍弃以节省显存开销。\nKV Cache有什么问题随着输入序列的增加，如果缓存所有的矩阵，那么对显存的需求将会二次上升。这就要求我们设计高效的KV Cache的动态调整策略。详细的KV Cache大小估算公式可以参考附录。\n这篇工作为什么叫StreamLLM，我的理解是因为即使随着对轮对话的进行，上下文越来越长，StreamLLM也能够基于注意力汇聚这一现象合理的关键的初始和一定上文窗口的cache来使得即使上下文增长也能像涓涓细流一样稳定流畅的推理。\nStreamLLM 的原理一些解决方式的对比🥵🥵🥵为了解决上文中提及的KV cache“爆炸”的现象，我们对比一些cache的策略：\n（a）最原始的解决“Dense Attention”\n所谓dense attention就是将所有的KV键值对全部存储起来\n\n\n\n\n这样存储不仅整体运算时间会达到，而且一旦tokens的数量超过了训练时候的最大长度，模型效果也会很差(用困惑度PPL来衡量，PPL越大模型效果越差)\n（b）自然而然想到的“windows attention”窗口注意力机制\n所谓窗口注意力机制即只缓存窗口长度为L的K值，如下图所示：\n\n\n\n\n这种窗口注意力机制能够将计算量降至，但是模型表现并不好（指困惑度很高\n（c）滑动窗口注意力机制“slide attention”\n\n\n\n\n所谓滑动窗口注意力的计算复杂度也是不可取的。\nStreamingLLM 技术和表现文中提出的StreamingLLM技术，保留了“下沉注意力”部分，又结合了最近的几个token的K值，就像下图所展示的一样。\n\n\n\n\n注：文中所说的保留“下沉注意力”是为了注意力分数的稳定计算。\n关于为什么会出现“注意力下沉”，以及为什么注意力下沉会出现在初始的token上，一种直观上的解释是：\n\n有很多token其实和前文关系都不大，但是在softmax操作中的归一化操作又必须保证注意力分数加和等于1；并且这其中又恰好只有初始的token才能被绝大多数后面的token看到.所以直观上注意力分数就应当会下沉于初始的token上；\n\n基于这些想法，我们就可以设计一个在有限窗口长度上训练的LLM有效的用于无限文本输入。\n实验验证 “attention sink” 现象如下图所示，下图是256个句子在LLaMa-2-7B上测试的平均注意力分数的热力图；很明显，注意力分布主要集中于recent tokens以及注意力下沉的initial token；\n\n\n\n\n以及在不同LLM上对不同策略的PPL测试结果：\n\n\n\n\n\n蓑鱿剪切线\n\n进一步探讨为什么窗口注意力会出现性能崩溃🧐🧐🧐在上图中已经展示过，当输入文本长度超过KV Cache的长度之后，PPL会出现激增现象。这恰好又从反面印证了：无论initial tokens距离当下的token距离有多远，他们对当下token注意力分数的影响都是重要的。\n从softmax函数的特性说起为什么initial token如此重要比较直观的，考虑softmax函数：\n\nSoftmax(x_i)=\\dfrac{e^{x_i}}{e^{x_1}+\\Sigma_{j=2}^{N}e^{x_j}},\\quad where \\quad x_1>>x_j;如果直接移除initial token，也就是公式中的，那么会导致大额注意力分数的重新分配，这必然会导致LLM的不稳定。\ninitial tokens重要的原因？语义信息or绝对位置？设计一个对比实验，将原本文本的initial tokens全部替换成”\\n”，会导致什么后果？\n\n\n\n\n结果并不出乎意料：替换之后的initial tokens依旧维持着高注意力分数，这意味着并不是initial tokens的语义信息让它们如此重要，它们重要的原因在于它们的初始的绝对位置。\n一些附加实验同样，作者还探究了initial tokens数量的选取对LLM效果的影响。\n\n\n\n\n\n不难发现选取1、2个initial tokens有时并不足以完全恢复LLM的效果。作者推测这是由于训练时候的预料并没有选取固定的开头前缀所导致的必然后果。\n可能的替代解决方案 softmax_off_by_one函数通过前文通篇的分析，我们不难得知：softmax函数的特性倾向于将注意力分数分配给全局可见的initial tokens，基于这点出发我们可以考虑将softmax进行改进，将其改写为：\n\nsoftmax_1(x_i)=\\dfrac{e^{x_i}}{1+\\Sigma_{j=1}^{N}e^{x_j}}这样改写等价于我们蓄意将一部分注意力分数提前拿出来进行sink。\n附录to be continue\n\n\n","categories":["学术"],"tags":["大模型加速","KV Cache 优化"]},{"title":"一个attention降秩理论的证明","url":"/post/d9fbe1c8.html","content":"Attention is not all you need？ 纯粹的注意力机制有什么问题？论文地址-&gt; pure attention loses rank doubly exponentially with depth &lt;-\n简介原文链接如上所示，论文开门见山的提出来一个新观点：纯粹的注意力机制会导致矩阵的秩指数级别的下降；论文标题也很有意思Attention is not all you need，则是与LLM的开山之作Attention is all you need相呼应，这篇文章看似在挑战attention机制，实际上是在从一个全新的角度来阐述为什么attention为什么会表现优异。\n回忆一下multi-head attention机制的细节：一个通俗且不严谨的科普（为了不懂NLP的观众）：在自然语言处理过程中，我们将每个word编码为一个vector（我们认为这个向量几何意义上会反映单词的语义信息，你可以理解为比如原神和崩坏铁道的向量表示相对距离更近、而和明日方舟更远，因为后者非米哈游产），从而单词组成的句子就会变成一个matrix。自然语言处理中有很多模块负责理解并处理这些matrix.\n\n\n\n\n\n上图就是一个多头注意力机制的原理图示。我们先尝试从数学的角度建模这个模块（真的很好理解、初中数学水平）：\n我们考虑一个输入是一个形如的输入。那么我们第h个注意力头的输出可以描述为：\nSA_{h}(\\mathcal{X})=P_{h}\\mathcal{X}W_{V,h}+1b_{V,h}^{T}其中，$W{V,h}是形如d{in}\\times d{v}$的value矩阵，$P{h}$是：\n\n\\begin{align}\nP_{h} &= \\text{softmax}\\left(d_{qk}^{-\\frac{1}{2}} \\left(\\mathcal{X}W_{Q,h} + 1b_{Q,h}^{T}\\right) \\left(\\mathcal{X}W_{K,h} + 1b_{K,h}^{T}\\right)^{T}\\right) \\\\\n&= \\text{softmax}\\left(d_{qk}^{-\\frac{1}{2}} \\left(\\mathcal{X}W_{QK,h}\\mathcal{X}^{T} + 1b_{Q,h}^{T}W_{K,h}^{T}\\mathcal{X}^{T}\\right)\\right)\n\\end{align}注意：这里的softmax操作是对矩阵的每一行进行的，$W{Q,h}、W{K,h}的形状都是d{in}\\times d{qk}，于是最后的输出是n\\times n的形状，并且根据的运算性质每行加上相同的值不会影响最终的输出，所以上述P_{h}$还可以接着作上述第二个等号的化简。如果你对这其中的某些步骤存在疑问可以关注后续会出一篇深度学习入门的博客文章。\n最后我们将多个头的注意力加权合并便得到最终这一层attention的输出：\n\n\\begin{align}\nSA(\\mathcal{X}) & =\\sum_{h\\in{[H]}}SA_h(\\mathcal{X})\\\\\n&=1[b_{O,1}^T,\\dots,b^{T}_{O,H}]+[SA_{1}(\\mathcal{X}),\\dots,SA_H(\\mathcal{X})][W_{O,1}^T,\\dots,W_{O,H}^T]^T\\\\\n&=\\sum_{h\\in[H]}P_h\\mathcal{X}W_h+1b_{O}^T\n\\end{align}其中，$Wh=W{V,h}W_{O,h}^T$;\n我们先忽略上面的偏置项，那我们一个由多层纯注意力层堆积而成的神经网络的最终输出可以描述为：\n\n\\begin{align}\n\\mathcal{X}^L &=\\sum_{h\\in [H_L]}P_{h}^{L}\\mathcal{X}^{L-1}W_{h}^L\\\\\n&=\\sum_{h\\in [H_L]}P_{h}^{L}(\\sum_{h^\\prime\\in[H_{L-1}]} P_{h^\\prime}^{L-1} \\mathcal{X}^{L-2} W_{h^\\prime}^{L-1})W_h^{L}\\\\\n&=\\sum_{h_1,\\dots,h_{L}\\in[H]^L}(P_{h_L}^L\\dots P^1_{h_1})\\mathcal{X}(W_{h_1}^1\\dots W_{h_L}^L)\n\\end{align}其实形象地，我们不难发现上述式子展开后的每一项都对应着多层注意力网络的一条可行路径（见下图。\n\n\n\n相信看完上述的描述之后，你肯定对线性LLM流行的多头注意力机制有了一个较为细致的了解了吧（不确信\npure attention collapse rank 现象？注意力降智降秩机制其实描述的是这样的事情：随着大模型层数的增加，如果我们简单的使用注意力层的堆叠，那么最后面的输出矩阵每行的向量指数级别的倾向于一致，也就是矩阵被降秩了！！！这对于LLM来说是一个非常糟糕的现象，毕竟谁都不希望看到自己的Chatbot只会说”啊对对对对对、啊错错错错错错错”吧。后面两个小节我们会分别从数学上证明这种现象和提出这种现象的解决方法\nMathematics Proof of Rank-Collapsing in pure ATTETION\n终于来到喜闻乐见的数学拷打时间了\n\n\n\n\n首先我们需要先定义一个残差，来衡量一个矩阵和秩①矩阵的相似程度，我们定义的残差如下：\n\nres(\\mathcal{X})=\\mathcal{X}-1x^T，where \\quad x=argmin_{x}\\|\\mathcal{X}-1x^T \\|不难验证，一个矩阵如果越越接近于秩①矩阵的话残差是越小的。并且从残差的定义来看（的任意性），偏置项是不会影响残差大小的。\n先来看单个头的单层注意力的情况对于单个头的一层注意力\n\n\\mathcal{X}^{\\prime}=SA(x)=P\\mathcal{X}W_V我们先来证明如下结论：\n\n\\|res(SA(\\mathcal{X}))\\|_{1,\\infty}\\leq \n\\dfrac{4\\gamma\\|W_{QK}\\|_{1} \\|W_V\\|_{1,\\infty}}{\\sqrt{d_{qk}}}\\|res(\\mathcal{X})\\|_{1,\\infty}^{3}其中是一个常量；\n由之前(2)式子的推导我们有：\n\nP(\\mathcal{X})=\\text{softmax}\\left(d_{qk}^{-\\frac{1}{2}} \\left(\\mathcal{X}W_{QK,h}\\mathcal{X}^{T} + 1b_{Q,h}^{T}W_{K,h}^{T}\\mathcal{X}^{T}\\right)\\right)我们引入记号，其中：\n\n\\begin{align}\n\\mathcal{A}&=\\mathcal{X}W_{QK}\\mathcal{X}^T+1b_{QK}^{T}\\mathcal{X}^T\\\\\n\\mathcal{R} &:= res(\\mathcal{X})\\\\\n\\mathcal{R^{\\prime}}&:=res(\\mathcal{X}^{\\prime})\n\\end{align}从而我们的注意力矩阵可以改写为：\n\n\\begin{align}\n\\mathcal{A}&=(1x^T+\\mathcal{R})\\dfrac{W_{QK}}{\\sqrt{d_{qk}}}(1x^T+\\mathcal{R})^T+1b_{QK}^{T}(1x^T+\\mathcal{R})^T\\\\\n&=(1x^T\\dfrac{W_{QK}}{\\sqrt{d_{qk}}}x+\\mathcal{R}\\dfrac{W_{QK}}{\\sqrt{d_{qk}}}x+1b_{QK}^{T}x)1^T + (1x^T+\\mathcal{R})\\dfrac{W_{QK}}{\\sqrt{d_{qk}}} + 1b_{QK}^T\\mathcal{R}^T\\\\\n\\end{align}我们再一次使用的平移不变的运算特性，可以得到：\n\n\\begin{align}\nP&=softmax(\\mathcal{R}\\dfrac{W_{QK}}{\\sqrt{d_{qk}}}\\mathcal{R}^T+1r^T)\\\\\nr&:=\\mathcal{R}\\dfrac{W_{QK}^T}{\\sqrt{d_{qk}}}x + \\mathcal{R}\\dfrac{b_{QK}}{\\sqrt{d_{qk}}}\n\n\\end{align}我们设$\\mathcal{E}:=\\mathcal{R}\\dfrac{W{QK}}{\\sqrt{d{qk}}}\\mathcal{R}^T、\\tilde{A}=1r^T$, 那么我们有：\n\n\\begin{align}\nP\\mathcal{X}&=P(1x^T+\\mathcal{R})\\\\\n&=1x^T+P\\mathcal{R}\\quad\\text{（这一步使用了softmax一行加和等于1的性质）}\\\\\n&=1x^T+softmax(\\mathcal{E}+1r^T)\\mathcal{R}\\\\\n&\\leq 1x^T+(I+2D)1sofmax(r)^T\\mathcal{R}\\quad\\text{（操蛋，这一步我没太看懂😤}\\\\\n&=1(x^T+softmax(r)^T\\mathcal{R})+2D1softmax(r)^T\\mathcal{R}\\\\\n\\end{align}$D矩阵的相关附录见Appendix的part1；\n😤😤😤不等式那一步我也没太看懂作者的意图，D是啥东西作者也没提，矩阵直接比较大小好像就是每个ij位置的元素对应比较。先硬着头皮看下去罢😤😤😤\n从而我们有：\n\n\\|[SA(\\mathcal{X})-1(r^\\prime)^T]_{ij}  \\| \\leq 2 \\| [ D1softmax(r)^TRW_{V}  ]_{ij}           \\|在此处;我们再来寻找上述不等式右边的界，考虑范数我们有：\n\n\\| [ D1softmax(r)^TRW_{V}  ]_{ij} \\|\\leq \\|D1\\|_1 \\|\\mathcal{R}\\|_1\\|W_V\\|_1在上述步骤中我们使用了以及的性质。从而不难得到的结论。\n通过类似的分析过程我们同样可以得到$|[SA(\\mathcal{X})-1(r^\\prime)^T]|\\infty\\leq2|D1|\\infty |\\mathcal{R}|\\infty|W_V|\\infty$.\n结合上述两步推导过程我们有：\n\n\\|\\mathcal{R}^\\prime\\|_{1,\\infty}=\\sqrt{\\|\\mathcal{R}^\\prime\\|_1\\|\\mathcal{R}^\\prime\\|_{\\infty}}\\leq2\\sqrt{\\|D1\\|_1 \\|D1\\|_\\infty}\\|\\mathcal{R}\\|_{1,\\infty}\\|W_{V}\\|_{1,\\infty}AppendixLemma-1引理1：设是矩阵的row-stochastic matrix，是矩阵的row-stochastic matrix.(for some matrix  with $|E{ij}-E{ij}^\\prime|\\leq 1.256$)，有：\n\n(I-D)\\tilde{P}\\leq P\\leq (I+2D)\\tilde{P}成立，其中对角矩阵满足$D{ii}=max{j,j^\\prime}|\\deltai^TE(\\delta_j-\\delta{j^\\prime})|（这里的\\delta_i$我猜测就是就是第i个元素为1的向量…\n剪切线，启动！\n\n\n\n后记\n这一篇论文过于理论化，Mr.Xau抽空前前后后一个星期才看完；内容太抽象导致本人也变得抽象起来了belike：\n\n","categories":["学术"],"tags":["数学理论"]},{"title":"最近想和大家讲讲`diffusion model`!","url":"/post/aa10f68a.html","content":"\r\nDiffusion和图像生成的关系\r\n谈到diffusion model那么就不得不谈及AIGC.\r\n在过去几年里里，以Stable\r\nDiffusion为代表的AI图像/视频生成是世界上最为火热的AI方向之一. Stable\r\nDiffusion里的这个”Diffusion”是什么意思？其实，扩散模型(Diffusion\r\nModel)正是Stable Diffusion中负责生成图像的模型。想要理解Stable\r\nDiffusion的原理，就一定绕不过扩散模型的学习。\r\n在这篇文章里，我会由浅入深地对去噪扩散概率模型（Denoising Diffusion\r\nProbabilistic Models, DDPM）进行一个介绍。\r\n图像生成任务的解决\r\n相比其他AI任务，图像生成任务显然是一个更加困难的事情.\r\n比如人脸识别,序列预测...这一系列任务都有明确的训练集来给出or蕴含一个[标准答案].\r\n但是图像生成就没有,\r\n图像生成数据集里只有一些同类型图片，却没有指导AI如何画得更好的信息。\r\n过去的解决方案:\r\nGAN对抗生成模型\r\n\r\nGAN的原理简介\r\n\r\nGAN的主要结构，包括一个生成器G（Generator）和一个判别器D（Discriminator），整个训练过程，便是二者的对抗博弈：\r\n给定参考数据集,\r\n希望学习出使得最优化下面的函数:\r\n\r\n它的含义其实就是: 对于生成模型,\r\n输入是随机噪声 ,\r\n输出为  , 上面第二项就是使\r\n 越能够迷惑判别器越好. 判别器\r\n 输入真实数据 or  判别器需要对两者进行辨别.\r\n\r\n\r\n\r\n\r\nGAN存在的问题:\r\n\r\n(*) 无法用于解决离散型数据的生成问题,\r\n自然语言处理是一个很典型的例子:\r\n局部信息很重要：图像局部很多细节并不太影响人类的对图像的理解，只要整体到位就\r\nok，不然也犯不着 CNN 这么多 filter\r\n一层层给你过滤，你破坏少数像素点不影响人类理解。自然语言麻烦在于，在细微处修改一下，就变味了。比如“西瓜汁好喝！”，我稍微改一下“西瓜汁好喝吗？”，尾巴动一点，整个意思都变了。GAN\r\n局部信息重构到底是靠死记硬背训练样本，还是靠神经网络插值“生成”出来的？我反正不清楚，不管如何，针对自然语言这种细节敏感的问题，GAN\r\n不是一个首选方案，不然 n-gram 的 LM 也不会活到今天。\r\n\r\n解决办法(引入强化学习RL)\r\n\r\nrelated works SeqGAN\r\nto be continued\r\n\r\nVAE\r\n(Variational AutoEncoder) 变分推断模型\r\nVAE作为可以和GAN比肩的生成模型，融合了贝叶斯方法和深度学习的优势，拥有优雅的数学基础和简单易懂的架构以及令人满意的性能，其能提取disentangled\r\nlatent variable的特性也使得它比一般的生成模型具有更广泛的意义。\r\n\r\n关于Latent Variable(隐藏变量)的理解\r\n\r\n生成模型一般会生成多个种类的数据，比如说在手写数字生成中，我们总共有10个类别的数字要生成，这个时候latent\r\nvariable model就是一个很好的选择。\r\n为什么呢？举例来说，我们很容易能注意到相同类别的数据在不同维度之间是有依赖存在的，比如生成数字5的时候，如果左边已经生成了数字5的左半部分，那么右半部分就几乎可以确定是5的另一半了。\r\n\r\n\r\n\r\n因此一个好的想法是，生成模型在生成数字的时候有两个步骤，即(1)决定要生成什么数字，这个数字用一个被称为latent\r\nvariable的向量z来表示，(2)然后再根据z来直接生成相应的数字。用数学表达式来表示就是：\r\n\r\n\r\n问:那么现在的关键是关于Latent Variable  的 先验概率分布形式  如何取值?\r\n\r\n答:很简单,直接设定 \r\n满足标准高斯分布就行.\r\n因为任何复杂的分布都可以通过多层MLP映射成标准高斯分布.\r\n\r\n问: 如何训练一个VAE\r\n\r\n答: 最大化  即可;\r\n\r\n有了的先验分布知识,我们可以使用若干次采样来最大化似然函数\r\n\r\n即最大化 \r\n然而当是维度很高的高斯分布的时候,这种方法训练十分低效.\r\n直接使用z先验分布来训练低效的原因直觉上是很明显的.因为对于数据集中的一个实例\r\n 而言,其对应的隐变量区间 \r\n实际上被似然函数采样到的概率是很低的.也就是说有效的训练次数很低.我们需要先假设一个\r\n 从此来针对数据集  先得到 \r\n来针对decoder训练,这样有效训练次数将大幅提升!\r\n或者换一种说法: 我们需要注意到, 对于采样  所有的  其实都是几乎为0的.\r\n换言之,绝大部分采样得到的的 \r\n对于目标函数 \r\n的贡献无足轻重. 在换言之,我们只需要关注  更大的部分即可.\r\n那么问题来了, 怎么计算 \r\n的后验知识 ??????\r\n很难的!\r\n\r\n\r\n\r\n\r\n贝叶斯公式巧妙转换 \r\n\r\n直接得到后验分布 \r\n是极其困难的,我们能够得到的只有encoder侧的输出 \r\n.我们需要记encoder的输出 ;但是与此同时必须保证  和 \r\n的分布相似性.这里用KL散度来衡量:\r\n\r\n使用贝叶斯公式对上式化简化繁:\r\n(其实贝叶斯这一步是最关键的一步)\r\n\r\n\r\n可以看见:我们通过使用贝叶斯公式将  巧妙地转换为\r\n\r\n将问题从encoder一侧转移到decoder一侧 !\r\n这是最最关键的一步!\r\n\r\n于是:\r\n\r\n再度化简可以得到 \r\n\r\n注意到KL散度的非负性,于是有:\r\n\r\n我们不妨记作:\r\n\r\nELBO(Variational Lower\r\nBound)记作变分下界;至此,我们近似将问题转化为了最大化变分下界;\r\n既然目标是让变分下界最大化，那么我们就需要仔细研究一下这个变分下界。\r\n\r\n首先是第一项，要想最大化\r\nELBO，那我们自然是想让第一项尽可能的大，也就是 x given z\r\n的概率分布期望值更大。这很明显就是由 z 到 x 重组的过程，也就是\r\nAutoEncoder 中的 Decoder，从潜在空间 Z 中重组\r\nx。模型想做的是尽可能准确地重组.\r\n其次是第二项，要想最大化 ELBO，我们自然需要让这项 KL\r\n散度尽可能小，也就是 潜在空间 z 的近似后验分布尽可能接近于 z\r\n的先验分布！这一项我们可以理解为，模型想让 z 尽可能避免过拟合.\r\n\r\n\r\nDiffusion模型\r\n扩散模型是一种特殊的VAE，其灵感来自于热力学：一个分布可以通过不断地添加噪声变成另一个分布。放到图像生成任务里，就是来自训练集的图像可以通过不断添加噪声变成符合标准正态分布的图像。但是:\r\n\r\n不再训练一个可学习的编码器，而是把编码过程固定成不断添加噪声的过程；\r\n不再把图像压缩成更短的向量，而是自始至终都对一个等大的图像做操作。解码器依然是一个可学习的神经网络，它的目的也同样是实现编码的逆操作。\r\n\r\n\r\n\r\n\r\n具体来说，扩散模型由正向过程和反向过程这两部分组成，对应VAE中的编码和解码。在正向过程中，输入\r\n 会不断混入高斯噪声. 经过  回合的加噪处理之后, 图像 \r\n会变成一个符合标准正态分布的纯噪声图像.\r\n而在反向过程中，我们希望训练出一个神经网络，该网络能够学会若干个去噪声操作，把\r\n 还原回  .\r\nPART1 加噪过程:\r\n前向加噪过程可以用描述为:\r\n\r\n\r\n其中 \r\n是高斯分布方差的超参数,在扩散过程中，随着  的增大, 越来越接近纯噪声。当  足够大的时候，收敛为标准高斯噪声 。\r\n不妨设  ,\r\n ,\r\n依次展开  可以得到:\r\n\r\n其中 , 由独立正态分布的可叠加性: \r\n:\r\n\r\n再进一步:\r\n\r\n这意味着 \r\n加噪过程到此结束.\r\nPART2 解噪过程\r\n实际上, 每一步降噪过程  是难以形式化求解的.\r\n我们的解码器就是为此而来的!其中  就是我们神经网络的参数:\r\n\r\n于是有:\r\n\r\n注意到(贝叶斯公式又立大功):\r\n\r\n第二个等号是因为这是Markov过程,后一个状态只取决于前一步状态.\r\n再次注意到:\r\n\r\n整理得到:\r\n\r\n其中:\r\n\r\n我们 \r\n需要拟合的就是上述的 .\r\n于是模型预测的 \r\n可以写作:\r\n\r\nPART3 训练过程\r\ndiffusion本质上是一种特殊的VAE model于是我们可以参考VAE变分推断的变分下界将问题进行转换:\r\n\r\n进一步处理可以得到:\r\n\r\n注意到加噪过程 \r\n是没有可以学习的参数的, 并且 \r\n近乎是纯高斯噪声, 于是上述第一项为常量,于是有:\r\n\r\n根据多元高斯分布的KL散度求解公式:\r\n\r\n代入:\r\n\r\n即可进行计算求解训练!\r\n\r\n\r\n\r\n\r\n光速进行一个条件化生成的介绍:\r\n作为生成模型，扩散模型和VAE、GAN、flow等模型的发展史很相似，都是先出来了无条件生成，然后有条件生成就紧接而来。无条件生成往往是为了探索效果上限，而有条件生成则更多是应用层面的内容，因为它可以实现根据我们的意愿来控制输出结果。\r\n从方法上来看，条件控制生成的方式分两种：事后修改(Classifier-Guidance)和事前训练(Classifier-Free)。对于大多数人来说，一个SOTA级别的扩散模型训练成本太大了，而分类器（Classifier）的训练还能接受，所以就想着直接复用别人训练好的无条件扩散模型，用一个分类器来调整生成过程以实现控制生成，这就是事后修改的Classifier-Guidance方案；\r\n\r\nClassifier-Guidance 条件控制方法\r\n\r\n无条件生成可以形式化描述为  , 加上条件  和分类器之后改写为  ,\r\n于是有:\r\n\r\n由于\r\n\r\n因此有:\r\n\r\n可以看到在经过一系列处理之后, 相比于原来无条件生成,仅仅多出来  一项. 对这一项进行控制,\r\n便可以实现控制生成方向.\r\n\r\n\r\n\r\n\r\n","categories":["学术"],"tags":["生成式模型","diffusion model"]},{"title":"强化学习基础","url":"/post/9109d8f5.html","content":"强化学习入门教程视频教程链接\n图文教程链接\n名词解释\n价值函数\n\n在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function）.于是我们可以将价值函数形式化的定义为:\n\nV(s)=\\mathscr{E}(G_t|S_t=s)展开可以得到:\n\n\\begin{align*}\nV(s)&=\\mathscr{E}(G_t|S_t=s)\\\\\n&=\\mathscr{E}(R_t+\\gamma R_{t+1}+{\\gamma}^2R_{t+2}+\\cdots+{\\gamma}^nR_{t+n}+\\cdots|S_t=s)\\\\\n&=\\mathscr{E}(R_t+\\gamma V(S_{t+1})|S_t=s)\n\\end{align*}根据马尔科夫概率转移过程我们可以将上面的式子改写为:\n\nV(s)=r(s)+\\gamma \\sum_{s^{\\prime}\\in S}p(s^{\\prime}|s)\\cdot V(s^{\\prime})\n贝尔曼方程(Bellman equation)\n\n若一个马尔科夫奖励过程一共有个状态,即:\n\nS=\\{s_1,s_2,\\cdots,s_n\\}将所有状态的价值表同样表示为一个向量的形式:\n\nV=\\{V(s_1),V(s_2),\\cdots,V(s_n)\\}^T同理将奖励函数写成一个列向量:\n\nR=\\{r(s_1),r(s_2),\\cdots,r(s_n)\\}^T于是我们可以将Bellman方程写为:\n\n\\begin{pmatrix}\nV(s_1) \\\\\nV(s_2) \\\\\n\\vdots \\\\\nV(s_n)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nr(s_1) \\\\\nr(s_2) \\\\\n\\vdots \\\\\nr(s_n)\n\\end{pmatrix}\n+\n\\gamma\n\\begin{pmatrix}\nP(s_1|s_1) & P(s_2|s_1) & \\cdots & P(s_n|s_1) \\\\\nP(s_1|s_2) & P(s_2|s_2) & \\cdots & P(s_n|s_2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nP(s_1|s_n) & P(s_2|s_n) & \\cdots & P(s_n|s_n)\n\\end{pmatrix}\n\\begin{pmatrix}\nV(s_1) \\\\\nV(s_2) \\\\\n\\vdots \\\\\nV(s_n)\n\\end{pmatrix}简记为:\n\nV=R+\\gamma PV可以直接求得解析解:\n\nV=(I-\\gamma P)^{-1}R上述计算式的计算复杂度是,对于大规模的马尔科夫奖励过程并不现实;解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划（dynamic programming）算法、蒙特卡洛方法（Monte-Carlo method）和时序差分（temporal difference）. 这些算法都将在后续计算.\n\n马尔科夫决策\n\n之前讨论到的马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（Markov decision process，MDP）。我们将这个来自外界的刺激称为智能体（agent）的动作.\nMDP可以由于一个五元组 $$ 来描述;相比于…\nto be continue\n经典强化学习策略REINFORCE-基于策略梯度的强化学习算法我们记一个马尔科夫序列的反馈值为:\n\nR(\\tau)=\\sum_{t=0}^{T-1}R(s_t,a_t)在策略梯度中，策略经常用一个带参数集的函数表示:.由策略决定的目标函数可以定义为:\n\nJ(\\pi_{\\theta})=E_{\\pi_{\\theta}}\\left(\\sum_{t=0}^{T-1}R(s_t,a_t)\\right)=E_{\\pi_{\\theta}}(R(\\tau))使用梯度上升策略来更新策略参数\n\n\\begin{align*}\n\\theta &=\\theta+\\alpha\\nabla E_{\\pi_{\\theta}}(R(\\tau))\\\\\n&=\\theta+\\alpha\\nabla E_{\\pi_{\\theta}}J(\\theta)\n\\end{align*}其中:\n\n\\begin{align*}\n\\nabla{J(\\theta)}&=\\nabla{\\left(\\sum_{\\tau|\\pi_{\\theta}}P(\\tau|\\theta)R(\\tau)\\right)}\\\\\n&=\\sum_{\\tau|\\pi_{\\theta}}\\left(P(\\tau|\\theta)\\cdot\\dfrac{\\nabla{P(\\tau|\\theta)}}{P(\\tau|\\theta)}\\cdot R(\\tau)\\right)\\\\\n&=E_{\\pi_\\theta}(\\nabla{log(P(\\tau|\\theta))\\cdot R(\\tau)})\\\\\n\\tau&=(\\{s_0,a_0\\},...\\{s_i,a_i\\},...)\n\\end{align*}而且其中又满足:\n\nP(\\tau|\\theta)= P(s_0)\\left[ \\prod_\\tau P(s_{i+1}|s_i,a_i)\\cdot \\pi_{\\theta}(a_i) \\right]对上式对数求导:\n\n\\begin{align*}\n\\nabla_{\\theta}{log(P(\\tau|\\theta))}&={\\nabla_{\\pi_\\theta}(logP(s_0))}+{\\nabla_\\theta\\left[\\sum_\\tau log(P(s_{i+1}|s_i,a_i))\\right]}+\\sum_{t=0}^{T-1}\\nabla_\\theta log(\\pi_\\theta(a_t,s_t))\\\\\n&=\\sum_{t=0}^{T-1}\\nabla_\\theta log(\\pi_\\theta(a_t,s_t))\\\\\n\\end{align*}于是有:\n\n\\begin{align*}\n\\nabla_{\\theta}(J(\\theta))&=E_{\\pi_\\theta}\\left(\\sum_{t=0}^{T-1}\\left[\\nabla_\\theta log(\\pi_\\theta(a_t,s_t))\\right]R(\\tau)\\right)\\\\\n&=\\sum_{\\tau}\\left(\\sum_{t=0}^{T-1}\\left[\\nabla_\\theta log(\\pi_\\theta(a_t,s_t))\\right]R(\\tau)\\right)\n\\end{align*}REINFORCE算法中的是基于蒙特卡洛采样的策略梯度方法采样得到的一个轨迹。也就是agent的采样策略是从开始状态一直到最终状态，是一个完整的轨迹。\n\n\n\n\nActor-Critic 算法在上述REINFORCE算法里面我们可以将简记为:\n\ng={E_\\tau}\\left(\\sum_{t=0}^{T-1}\\phi_{\\tau} \\cdot \\nabla_{\\theta}log\\pi_\\theta(a_t|s_t)\\right)其中可以取很多形式\n\n, 一条采样的总回报;\n$\\sum{t^\\prime=t}^{T}\\gamma^{t^\\prime-t}r{t^\\prime}采取动作a_{t^\\prime}$之后的回报;\n$\\sum{t^\\prime=t}^{T}\\gamma^{t^\\prime-t}r{t^\\prime}-b(s_t)$, 基准线版本的改进;\n\n\n\n$rt+\\gamma V^{\\pi\\theta}(s{t+1})-V^{\\pi\\theta}(s_t)$,时序差分残差;\n\n\n这个可以是任何我们想要给定策略参数想要达到的最大化或最小化的效应, 比如…\n提到 REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。我们可以用上述形式引入基线函数（baseline function）来减小方差。REINFORCE 算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。\nActor-Critic 算法顾名思义就是将强化学习算法分为两部分:\n\nCritic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。\n\n对于单个数据定义的如下的价值函数的损失函数:\n\n\\mathcal{L}(\\omega)=\\dfrac{1}{2}(r+\\gamma V_\\omega (s_{t+1})-V_\\omega(s_t))^2和DQN中一样,我们将上述$r+\\gamma V\\omega (s{t+1})$视作我们的差分目标,不会产生梯度来更新价值函数. 于是价值函数的梯度为:\n\n\\nabla_{\\omega}\\mathcal{L}(\\omega)=-(r+\\gamma V_{\\omega}(s_{t+1})-V_{\\omega}(s_t))\\nabla_{\\omega}V_{\\omega}(s_t)\nActor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。\n\n所以我们的Actor-Critic算法可以描述为:\n\n初始化策略网格参数以及价值网格参数\n\n使用当前的动作策略采样agent轨迹得到: \n\n为每一步数据计算$\\deltat=r_t+\\gamma V{\\omega}(s{t+1})-V{\\omega}(s_t)$\n\n更新价值网格参数$\\omega \\leftarrow:w+\\alpha{\\omega} \\sum{\\tau}\\deltat \\nabla{\\omega}V_{\\omega}(s_t)$\n\n根据更新之后的价值网格参数来更新策略网格参数$\\theta\\leftarrow:\\theta+\\alpha\\sum\\tau \\delta_t\\nabla{\\theta}{log(\\pi_\\theta(a_t|s_t))}$\n\n\nTRPO算法之前提到的两种基于策略的算法,在实际训练过程中极易出现训练不稳定的情况;\n具体回顾一下基于策略的算法策略:\n假设是策略的参数,定义价值函数:\n\nJ(\\theta)=E_{s_0}\\left[V^{\\pi_\\theta}(s_0) \\right]=E_{\\pi_\\theta}\\left[\\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)\\right]基于策略的方法的目标就是找到最优的;\n但是这种算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。\n针对以上问题，我们考虑在更新时找到一块信任区域（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是信任区域策略优化（trust region policy optimization，TRPO）算法的主要思想。\n具体来说考虑到初始状态的分布和我们的策略无关,所以我们的目标函数可以写做:\n\n\\begin{align*}\nJ(\\theta)&=E_{s_0}\\left[V^{\\pi_\\theta}(s_0)\\right]\\\\\n&=E_{\\pi_{\\theta^\\prime}}\\left[\\sum^{\\infty}_{t=0}\\gamma^tV^{\\pi_\\theta}(s_t)-\\sum_{t=1}^{\\infty}\\gamma^tV^{\\pi_\\theta}(s_t) \\right]\\\\\n&=E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\left[V^{\\pi_\\theta}(s_t)-\\gamma V^{\\pi_\\theta}(s_{t+1})  \\right]  \\right]\n\\end{align*}\\\\基于上述推导,我们可以得到新旧策略之间目标函数的差异:\n\n\\begin{align*}\n\\Delta J&=J(\\theta^\\prime)-J(\\theta)\\\\\n&=E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_t,a_t)\\right]-E_{\\pi_{\\theta^\\prime}}\\left[\\sum^{\\infty}_{t=0}\\gamma^tV^{\\pi_\\theta}(s_t)-\\sum_{t=1}^{\\infty}\\gamma^tV^{\\pi_\\theta}(s_t) \\right]\\\\\n&=E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_t,a_t)\\right]-E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\left[V^{\\pi_\\theta}(s_t)-\\gamma V^{\\pi_\\theta}(s_{t+1})  \\right]  \\right]\\\\\n&=E_{\\pi_{\\theta^\\prime}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^{t}\\left(r(s_t,a_t)+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_{t+1}) \\right) \\right]\n\\end{align*}如若定义残差函数:\n\nA(s_t,a_t)=r(s_t,a_t)+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_{t+1})注意看的第一项取决于新策略的状态访问分布,而后面两项取决于原始策略的状态访问分布;但是当我们策略更新步幅很小的时候,和的状态访问分布可以近似相等;\n我们定义:\n\n\\begin{align*}\nA&=E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}A^{\\pi_{\\theta}}(s_t,a_t)\\right]\\\\\n&=\\sum_{t=0}^\\infty\\left[\\gamma^t E_{s_t\\sim P_t^{\\pi_{\\theta^\\prime}}}(E_{a\\sim\\pi_{\\theta^\\prime}(\\cdot|s_t)}(A^{\\pi_{\\theta}}(s_t,a_t)))\\right]\\\\\n&=\\dfrac{1}{1-\\gamma}E_{s_t\\sim P_t^{\\pi_{\\theta^\\prime}}}(E_{a\\sim\\pi_{\\theta^\\prime}(\\cdot|s_t)}(A^{\\pi_{\\theta}}(s_t,a_t)))\\\\\n\\end{align*}于是我们的优化目标为:\n\n\\mathcal{L}(\\theta^\\prime)=\\mathcal{J}(\\theta)+\\dfrac{1}{1-\\gamma}E_{s_t\\sim P_t^{\\pi_{\\theta^\\prime}}}(E_{a\\sim\\pi_{\\theta^\\prime}(\\cdot|s_t)}(A^{\\pi_{\\theta}}(s_t,a_t)))进一步处理:\n\n\\mathcal{L}(\\theta^\\prime)=\\mathcal{J}(\\theta)+\\dfrac{1}{1-\\gamma}E_{s_t\\sim P_t^{\\pi_{\\theta}}}(E_{a\\sim\\pi_{\\theta}(\\cdot|s_t)}(\\dfrac{\\pi_{\\theta^\\prime(a|s)}}{\\pi_{\\theta (a|s)}} \\cdot A^{\\pi_{\\theta}}(s_t,a_t)))此外为了保证新旧策略足够接近，TRPO 使用了库尔贝克-莱布勒（Kullback-Leibler，KL）散度来衡量策略之间的距离.于是整体的优化公式修改为:\n\n\\begin{cases}\nmax_{\\theta^\\prime}\\left(L_{\\theta}(\\theta^\\prime)\\right)\\\\\ns.t. E_{s\\sim v^{\\pi_{\\theta_k}}}\\left[D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta^\\prime}(\\cdot|s))\\right]","categories":["学术"],"tags":["数学理论","强化学习","算法基础"]},{"title":"图论？绸带？算法？","url":"/post/36e56330.html","content":"记录最近遇到的一个关于排列与图论（？的算法题【题目简述】定义一个的排列为简洁，当且仅当均满足：\n现在给你一个任意的的排列,每一秒你可以交换其中任意两个元素的位置。试在最少的时间内把这个排列变得简洁，并且给出最少的步数。\n-&gt;【原题链接】&lt;- \n\n\n\n\n一些分析图从何而来？由于这是一个的排列，所以每个元素都会且只会出现一个，同理作为下标也只会出现一次；基于这点启发我们可以构造这样一个图：\n\n这个图有编号依次为的n个节点，如果有那么我们就添加一条从到的有向边。\n\n因为每个元素都会且只会出现一个，同理作为下标也只会出现一次，所以不难有每个点都只有一个入度和一个出度，也就是说这个这个图会由一系列简单的环组成。可以参考下面这个例子：\n\n\n\n\n\n事已至此，我们不难发现：在图中代表的就是一个点到自己的自环；对应的就是只有两个点彼此连接的环；所以我们的目标就变成了找到所有大于两个元素的环，然后把他们拆解为一系列两点环或者自环即可；\n\n一次交换操作会对图产生什么影响？我们还是按照上面的例子来做一个说明，如果我们对一个元素个数大于3的环做一次交换操作，那么我们可以做到从这个环里面拆出两点组成一个小环（这个小环）就对于情形2；最后面可能剩下1个或2个点都是能够满足条件的。\n所以我们的策略已经呼之欲出力！找到所有元素个数为m大于2的环，把他交换次即可；我们算法层面需要实现的就是找到所有环即可，不做赘述，请看代码；\n\n\n\n\n#include &lt;bits/stdc++.h&gt;using namespace\tstd;#define MAX_LEN 1000005 int\tmain(void){\tint case_num, array_length, arr[MAX_LEN];\tcin &gt;&gt; case_num; \tfor (int ii = 0; ii &lt; case_num; ii++)\t{\t\tcin &gt;&gt; array_length;\t\tbool used[array_length];\t\tfill(used, used + array_length, false);\t\tint ans = 0;\t\tvector&lt;int&gt; loops;\t\tfor (int jj = 0; jj &lt; array_length; jj++)\t\t{\t\t\tcin &gt;&gt; arr[jj];\t\t} \t\tint turns = 0;\t\tint start;\t\tfor (int kk = 0; kk &lt; array_length; kk++)\t\t{\t\t\tturns = 0;\t\t\tif (used[kk])\t\t\t{\t\t\t\tcontinue ;\t\t\t}\t\t\telse\t\t\t{\t\t\t\tused[kk] = true;\t\t\t\tturns++;\t\t\t\tstart = arr[kk] - 1; \t\t\t\twhile (!used[start])\t\t\t\t{\t\t\t\t\tused[start] = true;\t\t\t\t\tturns++;\t\t\t\t\tstart = arr[start] - 1;\t\t\t\t}\t\t\t\tloops.push_back(turns);\t\t\t}\t\t}\t\tfor (int i : loops)\t\t{\t\t\tans += (i - 1) / 2;\t\t\t// cout &lt;&lt; \"loops has \" &lt;&lt; i &lt;&lt; \" nodes\\n\";\t\t} \t\tcout &lt;&lt; ans &lt;&lt; endl;\t} \treturn (0);}\n\n\n\n\n\n    Accepted!!!\n\n\n\n\n\n","categories":["算法"],"tags":["刷题记录"]}]