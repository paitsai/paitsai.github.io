---
title: 强化学习基础
mathjax: true
categories: 学术
tags:
  - 强化学习
  - 算法基础
  - 数学理论
abbrlink: 9109d8f5
date: 2025-03-03 14:25:13
---


# 强化学习入门教程

[视频教程链接](https://www.bilibili.com/video/BV1sd4y167NS)

[图文教程链接](https://hrl.boyuai.com/chapter/1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B)


## 名词解释

> 价值函数

在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function）.
于是我们可以将价值函数形式化的定义为:

$$
V(s)=\mathscr{E}(G_t|S_t=s)
$$

展开可以得到:

$$
\begin{align*}
V(s)&=\mathscr{E}(G_t|S_t=s)\\
&=\mathscr{E}(R_t+\gamma R_{t+1}+{\gamma}^2R_{t+2}+\cdots+{\gamma}^nR_{t+n}+\cdots|S_t=s)\\
&=\mathscr{E}(R_t+\gamma V(S_{t+1})|S_t=s)
\end{align*}
$$

根据马尔科夫概率转移过程我们可以将上面的式子改写为:

$$
V(s)=r(s)+\gamma \sum_{s^{\prime}\in S}p(s^{\prime}|s)\cdot V(s^{\prime})
$$


> 贝尔曼方程(Bellman equation)


若一个马尔科夫奖励过程一共有$n$个状态,即:

$$
S=\{s_1,s_2,\cdots,s_n\}
$$

将所有状态的价值表同样表示为一个向量的形式:

$$
V=\{V(s_1),V(s_2),\cdots,V(s_n)\}^T
$$

同理将奖励函数写成一个列向量:

$$
R=\{r(s_1),r(s_2),\cdots,r(s_n)\}^T
$$

于是我们可以将`Bellman`方程写为:

$$
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{pmatrix}
=
\begin{pmatrix}
r(s_1) \\
r(s_2) \\
\vdots \\
r(s_n)
\end{pmatrix}
+
\gamma
\begin{pmatrix}
P(s_1|s_1) & P(s_2|s_1) & \cdots & P(s_n|s_1) \\
P(s_1|s_2) & P(s_2|s_2) & \cdots & P(s_n|s_2) \\
\vdots & \vdots & \ddots & \vdots \\
P(s_1|s_n) & P(s_2|s_n) & \cdots & P(s_n|s_n)
\end{pmatrix}
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{pmatrix}
$$



简记为:
$$
V=R+\gamma PV
$$

可以直接求得解析解:

$$
V=(I-\gamma P)^{-1}R
$$

上述计算式的计算复杂度是$\mathcal{O}(n^3)$,对于大规模的马尔科夫奖励过程并不现实;解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划（dynamic programming）算法、蒙特卡洛方法（Monte-Carlo method）和时序差分（temporal difference）. 这些算法都将在后续计算.

> 马尔科夫决策

之前讨论到的马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（Markov decision process，`MDP`）。我们将这个来自外界的刺激称为智能体（`agent`）的动作.

`MDP`可以由于一个五元组 $<S,A,P,\gamma,r>$ 来描述;相比于...

to be continue


## 经典强化学习策略

### REINFORCE-基于策略梯度的强化学习算法

我们记一个马尔科夫序列$\tau$的反馈值为:

$$
R(\tau)=\sum_{t=0}^{T-1}R(s_t,a_t)
$$

在策略梯度中，策略经常用一个带参数集$\theta$的函数表示:$\pi_{\theta}(a|s)$.由策略决定的目标函数可以定义为:

$$
J(\pi_{\theta})=E_{\pi_{\theta}}\left(\sum_{t=0}^{T-1}R(s_t,a_t)\right)=E_{\pi_{\theta}}(R(\tau))
$$

使用梯度上升策略来更新策略参数$\theta$

$$
\begin{align*}
\theta &=\theta+\alpha\nabla E_{\pi_{\theta}}(R(\tau))\\
&=\theta+\alpha\nabla E_{\pi_{\theta}}J(\theta)
\end{align*}
$$

其中:

$$
\begin{align*}
\nabla{J(\theta)}&=\nabla{\left(\sum_{\tau|\pi_{\theta}}P(\tau|\theta)R(\tau)\right)}\\
&=\sum_{\tau|\pi_{\theta}}\left(P(\tau|\theta)\cdot\dfrac{\nabla{P(\tau|\theta)}}{P(\tau|\theta)}\cdot R(\tau)\right)\\
&=E_{\pi_\theta}(\nabla{log(P(\tau|\theta))\cdot R(\tau)})\\
\tau&=(\{s_0,a_0\},...\{s_i,a_i\},...)
\end{align*}
$$

而且其中又满足:

$$
P(\tau|\theta)= P(s_0)\left[ \prod_\tau P(s_{i+1}|s_i,a_i)\cdot \pi_{\theta}(a_i) \right]
$$

对上式对数求导:

$$
\begin{align*}
\nabla_{\theta}{log(P(\tau|\theta))}&={\nabla_{\pi_\theta}(logP(s_0))}+{\nabla_\theta\left[\sum_\tau log(P(s_{i+1}|s_i,a_i))\right]}+\sum_{t=0}^{T-1}\nabla_\theta log(\pi_\theta(a_t,s_t))\\
&=\sum_{t=0}^{T-1}\nabla_\theta log(\pi_\theta(a_t,s_t))\\
\end{align*}
$$

于是有:

$$
\begin{align*}
\nabla_{\theta}(J(\theta))&=E_{\pi_\theta}\left(\sum_{t=0}^{T-1}\left[\nabla_\theta log(\pi_\theta(a_t,s_t))\right]R(\tau)\right)\\
&=\sum_{\tau}\left(\sum_{t=0}^{T-1}\left[\nabla_\theta log(\pi_\theta(a_t,s_t))\right]R(\tau)\right)
\end{align*}
$$

REINFORCE算法中的$\tau$是基于蒙特卡洛采样的策略梯度方法采样得到的一个轨迹。也就是agent的采样策略是从开始状态一直到最终状态，是一个完整的轨迹。


<center>
<img src="/pics/tau_guiji.png" width="80%">
</center>

### Actor-Critic 算法


在上述REINFORCE算法里面我们可以将$\nabla_\theta{J(\theta)}$简记为:

$$
g={E_\tau}\left(\sum_{t=0}^{T-1}\phi_{\tau} \cdot \nabla_{\theta}log\pi_\theta(a_t|s_t)\right)
$$

其中$\phi_\tau$可以取很多形式

- $\sum_{\tau}\gamma^tr_t$, 一条采样的总回报;
- $\sum_{t^\prime=t}^{T}\gamma^{t^\prime-t}r_{t^\prime}$, 采取动作$a_{t^\prime}$之后的回报;
- $\sum_{t^\prime=t}^{T}\gamma^{t^\prime-t}r_{t^\prime}-b(s_t)$
, 基准线版本的改进;

- $\cdots$

- $r_t+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t)$,时序差分残差;


这个$\phi_\tau$可以是任何我们想要给定策略参数$\theta$想要达到的最大化或最小化的`效应`, 比如...


提到 REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。我们可以用上述形式引入基线函数$b(s_t)$（baseline function）来减小方差。REINFORCE 算法基于蒙特卡洛采样，只能在`序列结束后进行更新`，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。

Actor-Critic 算法顾名思义就是将强化学习算法分为两部分:

- Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。

对于单个数据定义的如下的价值函数的损失函数:

$$
\mathcal{L}(\omega)=\dfrac{1}{2}(r+\gamma V_\omega (s_{t+1})-V_\omega(s_t))^2
$$

和DQN中一样,我们将上述$r+\gamma V_\omega (s_{t+1})$视作我们的差分目标,不会产生梯度来更新价值函数. 于是价值函数的梯度为:

$$
\nabla_{\omega}\mathcal{L}(\omega)=-(r+\gamma V_{\omega}(s_{t+1})-V_{\omega}(s_t))\nabla_{\omega}V_{\omega}(s_t)
$$


- Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。

所以我们的`Actor-Critic`算法可以描述为:

- 初始化策略网格参数$\theta$以及价值网格参数$\omega$

- 使用当前的动作策略$\pi_{\theta_0}$采样agent轨迹得到$\tau_0$: $\left[ (s_1,a_1),\cdots(s_i,a_i),\cdots \right]$

- 为每一步数据计算$\delta_t=r_t+\gamma V_{\omega}(s_{t+1})-V_{\omega}(s_t)$

- 更新价值网格参数$\omega \leftarrow:w+\alpha_{\omega} \sum_{\tau}\delta_t \nabla_{\omega}V_{\omega}(s_t)$

- 根据更新之后的价值网格参数来更新策略网格参数$\theta\leftarrow:\theta+\alpha\sum_\tau \delta_t\nabla_{\theta}{log(\pi_\theta(a_t|s_t))}$


### `TRPO`算法


之前提到的两种基于策略的算法,在实际训练过程中极易出现训练不稳定的情况;

具体回顾一下基于策略的算法策略:



假设$\theta$是策略$\pi_\theta$的参数,定义价值函数:

$$
J(\theta)=E_{s_0}\left[V^{\pi_\theta}(s_0) \right]=E_{\pi_\theta}\left[\sum_{t=0}^\infty \gamma^t r(s_t,a_t)\right]
$$

基于策略的方法的目标就是找到最优的$\theta^*=\argmax_\theta J(\theta)$;

但是这种算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。

针对以上问题，我们考虑在更新时找到一块`信任区域（trust region）`，在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是信任区域策略优化（trust region policy optimization，TRPO）算法的主要思想。


具体来说考虑到**初始状态$S_0$的分布和我们的策略无关**,所以我们的目标函数可以写做:

$$
\begin{align*}
J(\theta)&=E_{s_0}\left[V^{\pi_\theta}(s_0)\right]\\
&=E_{\pi_{\theta^\prime}}\left[\sum^{\infty}_{t=0}\gamma^tV^{\pi_\theta}(s_t)-\sum_{t=1}^{\infty}\gamma^tV^{\pi_\theta}(s_t) \right]\\
&=E_{\pi_{\theta^\prime}}\left[\sum_{t=0}^{\infty}\gamma^t\left[V^{\pi_\theta}(s_t)-\gamma V^{\pi_\theta}(s_{t+1})  \right]  \right]
\end{align*}\\
$$

基于上述推导,我们可以得到新旧策略之间目标函数的差异:

$$
\begin{align*}
\Delta J&=J(\theta^\prime)-J(\theta)\\
&=E_{\pi_{\theta^\prime}}\left[\sum_{t=0}^{\infty}\gamma^{t}r(s_t,a_t)\right]-E_{\pi_{\theta^\prime}}\left[\sum^{\infty}_{t=0}\gamma^tV^{\pi_\theta}(s_t)-\sum_{t=1}^{\infty}\gamma^tV^{\pi_\theta}(s_t) \right]\\
&=E_{\pi_{\theta^\prime}}\left[\sum_{t=0}^{\infty}\gamma^{t}r(s_t,a_t)\right]-E_{\pi_{\theta^\prime}}\left[\sum_{t=0}^{\infty}\gamma^t\left[V^{\pi_\theta}(s_t)-\gamma V^{\pi_\theta}(s_{t+1})  \right]  \right]\\
&=E_{\pi_{\theta^\prime}}\left[ \sum_{t=0}^{\infty}\gamma^{t}\left(r(s_t,a_t)+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_{t+1}) \right) \right]
\end{align*}
$$


如若定义残差函数:

$$
A(s_t,a_t)=r(s_t,a_t)+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_{t+1})  
$$

注意看$A(s_t,a_t)$的第一项取决于新策略$\theta^\prime$的状态访问分布,而后面两项取决于原始策略$\theta$的状态访问分布;但是当我们策略更新步幅很小的时候,$\theta^\prime$和$\theta$的状态访问分布可以近似相等;


我们定义:

$$
\begin{align*}
A&=E_{\pi_{\theta^\prime}}\left[\sum_{t=0}^{\infty}\gamma^{t}A^{\pi_{\theta}}(s_t,a_t)\right]\\
&=\sum_{t=0}^\infty\left[\gamma^t E_{s_t\sim P_t^{\pi_{\theta^\prime}}}(E_{a\sim\pi_{\theta^\prime}(\cdot|s_t)}(A^{\pi_{\theta}}(s_t,a_t)))\right]\\
&=\dfrac{1}{1-\gamma}E_{s_t\sim P_t^{\pi_{\theta^\prime}}}(E_{a\sim\pi_{\theta^\prime}(\cdot|s_t)}(A^{\pi_{\theta}}(s_t,a_t)))\\
\end{align*}
$$


于是我们的优化目标为:

$$
\mathcal{L}(\theta^\prime)=\mathcal{J}(\theta)+\dfrac{1}{1-\gamma}E_{s_t\sim P_t^{\pi_{\theta^\prime}}}(E_{a\sim\pi_{\theta^\prime}(\cdot|s_t)}(A^{\pi_{\theta}}(s_t,a_t)))
$$

进一步处理:

$$
\mathcal{L}(\theta^\prime)=\mathcal{J}(\theta)+\dfrac{1}{1-\gamma}E_{s_t\sim P_t^{\pi_{\theta}}}(E_{a\sim\pi_{\theta}(\cdot|s_t)}(\dfrac{\pi_{\theta^\prime(a|s)}}{\pi_{\theta (a|s)}} \cdot A^{\pi_{\theta}}(s_t,a_t)))
$$


此外为了保证新旧策略足够接近，TRPO 使用了库尔贝克-莱布勒（Kullback-Leibler，KL）散度来衡量策略之间的距离.于是整体的优化公式修改为:


$$
\begin{cases}
max_{\theta^\prime}\left(L_{\theta}(\theta^\prime)\right)\\
s.t. E_{s\sim v^{\pi_{\theta_k}}}\left[D_{KL}(\pi_{\theta_k}(\cdot|s),\pi_{\theta^\prime}(\cdot|s))\right]<\delta.
\end{cases}
$$

这里的不等式约束定义了策略空间中的一个 KL 球，被称为信任区域。在这个区域中，可以认为当前学习策略和环境交互的状态分布与上一轮策略最后采样的状态分布一致，进而可以基于一步行动的重要性采样方法使当前学习策略稳定提升。

<center>
<img src="https://hrl.boyuai.com/static/640.50a56fc9.png" width="60%">
</center>


### `PPO`算法


介绍的 TRPO 算法在很多场景上的应用都很成功，但是我们也发现它的计算过程非常复杂，每一步更新的运算量非常大。于是，TRPO 算法的改进版——PPO 算法在 2017 年被提出，PPO 基于 TRPO 的思想，但是其算法实现更加简单。


让我们回忆一下`TRPO`的优化目标:

$$
\begin{align*}
max_{\theta^\prime}\quad &E_{s\sim v^{\pi_{\theta_k}}}\left(E_{a\sim \pi_{\pi_{\theta_k}}}\left[\dfrac{\pi_{\theta^\prime}(a|s)}{\pi_{\theta_k}(a|s)}\cdot A(s,a)  \right] \right)\\
s.t.\quad&E_{s\sim v^{\pi_{\theta_k}}}\left[D_{KL}\left(\pi_{\theta_k}(\cdot|s),\pi_{\theta^{\prime}}(\cdot|s) \right) \right]
\end{align*}
$$

TRPO 使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。但 PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断，我们接下来对这两种形式进行介绍。

> `PPO`-惩罚算法


PPO-惩罚（PPO-Penalty）用拉格朗日乘数法直接将 KL 散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数。即：


$$
\argmax_{\theta}E_{s\sim v^{\theta_k}(\cdot|s)}E_{a\sim \pi_{\theta_k}(\cdot|s)}\left[\dfrac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)-\beta D_{KL}\left(\pi_{\theta_k}(\cdot|s),\pi_{\theta^{\prime}}(\cdot|s) \right)   \right]
$$


设$d_k=D_{KL}\left(\pi_{\theta_k}(\cdot|s),\pi_{\theta^{\prime}}(\cdot|s) \right)$, 那么$\beta$的更新规则如下:

- 如果$d_k<\dfrac{\delta}{1.5}$,那么$\beta_{k+1}\leftarrow\dfrac{\beta_k}{2}$.
- 如果$d_k>1.5\delta$,那么$\beta_{k+1}\leftarrow{\beta_k}\times{2}$.
- 否则$\beta_{k+1}=\beta_k$.

其中，$\delta$是事先设定的一个超参数，用于限制学习策略和之前一轮策略的差距。





> `PPO`截断


PPO 的另一种形式 PPO-截断（PPO-Clip）更加直接，它在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大，即：

$$
\argmax_{\theta}E_{s\sim v^{\theta_k}(\cdot|s)}E_{a\sim \pi_{\theta_k}(\cdot|s)}\left[\mathcal{Clip}\left(\dfrac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon \right)A^{\pi_{\theta_k}}(s,a)  \right]
$$







<center>
<img src="/pics/sk_gl.jpg" width="40%">
</center>