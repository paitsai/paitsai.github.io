---
title: 强化学习基础
mathjax: true
date: 2025-03-03 14:25:13
categories: 学术
tags:
    - 强化学习
    - 算法基础
    - 数学理论
---


# 强化学习入门教程

[视频教程链接](https://www.bilibili.com/video/BV1sd4y167NS)

[图文教程链接](https://hrl.boyuai.com/chapter/1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B)


## 名词解释

> 价值函数

在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function）.
于是我们可以将价值函数形式化的定义为:

$$
V(s)=\mathscr{E}(G_t|S_t=s)
$$

展开可以得到:

$$
\begin{align*}
V(s)&=\mathscr{E}(G_t|S_t=s)\\
&=\mathscr{E}(R_t+\gamma R_{t+1}+{\gamma}^2R_{t+2}+\cdots+{\gamma}^nR_{t+n}+\cdots|S_t=s)\\
&=\mathscr{E}(R_t+\gamma V(S_{t+1})|S_t=s)
\end{align*}
$$

根据马尔科夫概率转移过程我们可以将上面的式子改写为:

$$
V(s)=r(s)+\gamma \sum_{s^{\prime}\in S}p(s^{\prime}|s)\cdot V(s^{\prime})
$$


> 贝尔曼方程(Bellman equation)


若一个马尔科夫奖励过程一共有$n$个状态,即:

$$
S=\{s_1,s_2,\cdots,s_n\}
$$

将所有状态的价值表同样表示为一个向量的形式:

$$
V=\{V(s_1),V(s_2),\cdots,V(s_n)\}^T
$$

同理将奖励函数写成一个列向量:

$$
R=\{r(s_1),r(s_2),\cdots,r(s_n)\}^T
$$

于是我们可以将`Bellman`方程写为:

$$
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\cdots\\
V(s_n)
\end{pmatrix}
=
\begin{pmatrix}
r(s_1) \\
r(s_2) \\
\cdots\\
r(s_n)
\end{pmatrix}
+
\gamma \begin{pmatrix}
P(s_1|s_1), P(s_2|s_1),\cdots,P(s_n|s_1)  \\
P(s_1|s_2), P(s_2|s_2),\cdots,P(s_n|s_2) \\
\cdots\\
P(s_1|s_n), P(s_2|s_n),\cdots,P(s_n|s_n)
\end{pmatrix}\cdot
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\cdots\\
V(s_n)
\end{pmatrix}

$$

简记为:
$$
V=R+\gamma PV
$$

可以直接求得解析解:

$$
V=(I-\gamma P)^{-1}R
$$

上述计算式的计算复杂度是$\mathcal{O}(n^3)$,对于大规模的马尔科夫奖励过程并不现实;解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划（dynamic programming）算法、蒙特卡洛方法（Monte-Carlo method）和时序差分（temporal difference）. 这些算法都将在后续计算.

> 马尔科夫决策

之前讨论到的马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（Markov decision process，`MDP`）。我们将这个来自外界的刺激称为智能体（`agent`）的动作.

`MDP`可以由于一个五元组 $<S,A,P,\gamma,r>$ 来描述;相比于...

to be continue

> REINFORCE-基于策略梯度的强化学习算法

我们记一个马尔科夫序列$\tau$的反馈值为:

$$
R(\tau)=\sum_{t=0}^{T-1}R(s_t,a_t)
$$

在策略梯度中，策略经常用一个带参数集$\theta$的函数表示:$\pi_{\theta}(a|s)$.由策略决定的目标函数可以定义为:

$$
J(\pi_{\theta})=E_{\pi_{\theta}}\left(\sum_{t=0}^{T-1}R(s_t,a_t)\right)=E_{\pi_{\theta}}(R(\tau))
$$

使用梯度上升策略来更新策略参数$\theta$

$$
\begin{align*}
\theta &=\theta+\alpha\nabla E_{\pi_{\theta}}(R(\tau))\\
&=\theta+\alpha\nabla E_{\pi_{\theta}}J(\theta)
\end{align*}
$$

其中:

$$
\begin{align*}
\nabla{J(\theta)}&=\nabla{\left(\sum_{\tau|\pi_{\theta}}P(\tau|\theta)R(\tau)\right)}\\
&=\sum_{\tau|\pi_{\theta}}\left(P(\tau|\theta)\cdot\dfrac{\nabla{P(\tau|\theta)}}{P(\tau|\theta)}\cdot R(\tau)\right)\\
&=E_{\pi_\theta}(\nabla{log(P(\tau|\theta))\cdot R(\tau)})\\
\tau&=(\{s_0,a_0\},...\{s_i,a_i\},...)
\end{align*}
$$

而且其中又满足:

$$
P(\tau|\theta)= P(s_0)\left[ \prod_\tau P(s_{i+1}|s_i,a_i)\cdot \pi_{\theta}(a_i) \right]
$$

对上式对数求导:

$$
\nabla_{\theta}{log(P(\tau|\theta))}=\sout{\nabla_{\pi_\theta}(logP(s_0))}+\sout{\nabla_\theta\left[\sum_\tau log(P(s_{i+1}|s_i,a_i))\right]}+\sum_{t=0}^{T-1}\nabla_\theta log(\pi_\theta(a_t,s_t))\\
\nabla_{\theta}(J(\theta))=E_{\pi_\theta}\left(\sum_{t=0}^{T-1}\left[\nabla_\theta log(\pi_\theta(a_t,s_t))\right]R(\tau)\right)=
\sum_{\tau}\left(\sum_{t=0}^{T-1}\left[\nabla_\theta log(\pi_\theta(a_t,s_t))\right]R(\tau)\right)
$$

REINFORCE算法中的$\tau$是基于蒙特卡洛采样的策略梯度方法采样得到的一个硅基。也就是agent的采样策略是从开始状态一直到最终状态，是一个完整的轨迹。


<center>
<img src="/pics/tau_guiji.png" width="80%">
</center>





