---
title: 强化学习基础
mathjax: true
categories: 学术
tags:
  - 强化学习
  - 算法基础
  - 数学理论
abbrlink: 9109d8f5
date: 2025-03-03 14:25:13
---


# 强化学习入门教程

[视频教程链接](https://www.bilibili.com/video/BV1sd4y167NS)

[图文教程链接](https://hrl.boyuai.com/chapter/1/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B)


## 名词解释

> 价值函数

在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function）.
于是我们可以将价值函数形式化的定义为:

$$
V(s)=\mathscr{E}(G_t|S_t=s)
$$

展开可以得到:

$$
\begin{align*}
V(s)&=\mathscr{E}(G_t|S_t=s)\\
&=\mathscr{E}(R_t+\gamma R_{t+1}+{\gamma}^2R_{t+2}+\cdots+{\gamma}^nR_{t+n}+\cdots|S_t=s)\\
&=\mathscr{E}(R_t+\gamma V(S_{t+1})|S_t=s)
\end{align*}
$$

根据马尔科夫概率转移过程我们可以将上面的式子改写为:

$$
V(s)=r(s)+\gamma \sum_{s^{\prime}\in S}p(s^{\prime}|s)\cdot V(s^{\prime})
$$


> 贝尔曼方程(Bellman equation)


若一个马尔科夫奖励过程一共有$n$个状态,即:

$$
S=\{s_1,s_2,\cdots,s_n\}
$$

将所有状态的价值表同样表示为一个向量的形式:

$$
V=\{V(s_1),V(s_2),\cdots,V(s_n)\}^T
$$

同理将奖励函数写成一个列向量:

$$
R=\{r(s_1),r(s_2),\cdots,r(s_n)\}^T
$$

于是我们可以将`Bellman`方程写为:

$$
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{pmatrix}
=
\begin{pmatrix}
r(s_1) \\
r(s_2) \\
\vdots \\
r(s_n)
\end{pmatrix}
+
\gamma
\begin{pmatrix}
P(s_1|s_1) & P(s_2|s_1) & \cdots & P(s_n|s_1) \\
P(s_1|s_2) & P(s_2|s_2) & \cdots & P(s_n|s_2) \\
\vdots & \vdots & \ddots & \vdots \\
P(s_1|s_n) & P(s_2|s_n) & \cdots & P(s_n|s_n)
\end{pmatrix}
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\vdots \\
V(s_n)
\end{pmatrix}
$$

<!-- $$
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\cdots\\
V(s_n)
\end{pmatrix}=\begin{pmatrix}
r(s_1) \\
r(s_2) \\
\cdots\\
r(s_n)
\end{pmatrix}+\gamma \begin{pmatrix}
P(s_1|s_1), P(s_2|s_1),\cdots,P(s_n|s_1)  \\
P(s_1|s_2), P(s_2|s_2),\cdots,P(s_n|s_2) \\
\cdots\\
P(s_1|s_n), P(s_2|s_n),\cdots,P(s_n|s_n)
\end{pmatrix}\cdot
\begin{pmatrix}
V(s_1) \\
V(s_2) \\
\cdots\\
V(s_n)
\end{pmatrix}

$$ -->

简记为:
$$
V=R+\gamma PV
$$

可以直接求得解析解:

$$
V=(I-\gamma P)^{-1}R
$$

上述计算式的计算复杂度是$\mathcal{O}(n^3)$,对于大规模的马尔科夫奖励过程并不现实;解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划（dynamic programming）算法、蒙特卡洛方法（Monte-Carlo method）和时序差分（temporal difference）. 这些算法都将在后续计算.

> 马尔科夫决策

之前讨论到的马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（Markov decision process，`MDP`）。我们将这个来自外界的刺激称为智能体（`agent`）的动作.

`MDP`可以由于一个五元组 $<S,A,P,\gamma,r>$ 来描述;相比于...

to be continue


## 经典强化学习策略

> REINFORCE-基于策略梯度的强化学习算法

我们记一个马尔科夫序列$\tau$的反馈值为:

$$
R(\tau)=\sum_{t=0}^{T-1}R(s_t,a_t)
$$

在策略梯度中，策略经常用一个带参数集$\theta$的函数表示:$\pi_{\theta}(a|s)$.由策略决定的目标函数可以定义为:

$$
J(\pi_{\theta})=E_{\pi_{\theta}}\left(\sum_{t=0}^{T-1}R(s_t,a_t)\right)=E_{\pi_{\theta}}(R(\tau))
$$

使用梯度上升策略来更新策略参数$\theta$

$$
\begin{align*}
\theta &=\theta+\alpha\nabla E_{\pi_{\theta}}(R(\tau))\\
&=\theta+\alpha\nabla E_{\pi_{\theta}}J(\theta)
\end{align*}
$$

其中:

$$
\begin{align*}
\nabla{J(\theta)}&=\nabla{\left(\sum_{\tau|\pi_{\theta}}P(\tau|\theta)R(\tau)\right)}\\
&=\sum_{\tau|\pi_{\theta}}\left(P(\tau|\theta)\cdot\dfrac{\nabla{P(\tau|\theta)}}{P(\tau|\theta)}\cdot R(\tau)\right)\\
&=E_{\pi_\theta}(\nabla{log(P(\tau|\theta))\cdot R(\tau)})\\
\tau&=(\{s_0,a_0\},...\{s_i,a_i\},...)
\end{align*}
$$

而且其中又满足:

$$
P(\tau|\theta)= P(s_0)\left[ \prod_\tau P(s_{i+1}|s_i,a_i)\cdot \pi_{\theta}(a_i) \right]
$$

对上式对数求导:

$$
\begin{align*}
\nabla_{\theta}{log(P(\tau|\theta))}&={\nabla_{\pi_\theta}(logP(s_0))}+{\nabla_\theta\left[\sum_\tau log(P(s_{i+1}|s_i,a_i))\right]}+\sum_{t=0}^{T-1}\nabla_\theta log(\pi_\theta(a_t,s_t))\\
&=\sum_{t=0}^{T-1}\nabla_\theta log(\pi_\theta(a_t,s_t))\\
\end{align*}
$$

于是有:

$$
\begin{align*}
\nabla_{\theta}(J(\theta))&=E_{\pi_\theta}\left(\sum_{t=0}^{T-1}\left[\nabla_\theta log(\pi_\theta(a_t,s_t))\right]R(\tau)\right)\\
&=\sum_{\tau}\left(\sum_{t=0}^{T-1}\left[\nabla_\theta log(\pi_\theta(a_t,s_t))\right]R(\tau)\right)
\end{align*}
$$

REINFORCE算法中的$\tau$是基于蒙特卡洛采样的策略梯度方法采样得到的一个轨迹。也就是agent的采样策略是从开始状态一直到最终状态，是一个完整的轨迹。


<center>
<img src="/pics/tau_guiji.png" width="80%">
</center>

> Actor-Critic 算法


在上述REINFORCE算法里面我们可以将$\nabla_\theta{J(\theta)}$简记为:

$$
g={E_\tau}\left(\sum_{t=0}^{T-1}\phi_{\tau} \cdot \nabla_{\theta}log\pi_\theta(a_t|s_t)\right)
$$

其中$\phi_\tau$可以取很多形式

- $\sum_{\tau}\gamma^tr_t$, 一条采样的总回报;
- $\sum_{t^\prime=t}^{T}\gamma^{t^\prime-t}r_{t^\prime}$, 采取动作$a_{t^\prime}$之后的回报;
- $\sum_{t^\prime=t}^{T}\gamma^{t^\prime-t}r_{t^\prime}-b(s_t)$
, 基准线版本的改进;

- $\cdots$

- $r_t+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t)$,时序差分残差;


这个$\phi_\tau$可以是任何我们想要给定策略参数$\theta$想要达到的最大化或最小化的`效应`, 比如...


提到 REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。我们可以用上述形式引入基线函数$b(s_t)$（baseline function）来减小方差。REINFORCE 算法基于蒙特卡洛采样，只能在`序列结束后进行更新`，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。

Actor-Critic 算法顾名思义就是将强化学习算法分为两部分:

- Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。

对于单个数据定义的如下的价值函数的损失函数:

$$
\mathcal{L}(\omega)=\dfrac{1}{2}(r+\gamma V_\omega (s_{t+1})-V_\omega(s_t))^2
$$

和DQN中一样,我们将上述$r+\gamma V_\omega (s_{t+1})$视作我们的差分目标,不会产生梯度来更新价值函数. 于是价值函数的梯度为:

$$
\nabla_{\omega}\mathcal{L}(\omega)=-(r+\gamma V_{\omega}(s_{t+1})-V_{\omega}(s_t))\nabla_{\omega}V_{\omega}(s_t)
$$


- Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。

所以我们的`Actor-Critic`算法可以描述为:

- 初始化策略网格参数$\theta$以及价值网格参数$\omega$

- 使用当前的动作策略$\pi_{\theta_0}$采样agent轨迹得到$\tau_0$: $\left[ (s_1,a_1),\cdots(s_i,a_i),\cdots \right]$

- 为每一步数据计算$\delta_t=r_t+\gamma V_{\omega}(s_{t+1})-V_{\omega}(s_t)$

- 更新价值网格参数$\omega \leftarrow:w+\alpha_{\omega} \sum_{\tau}\delta_t \nabla_{\omega}V_{\omega}(s_t)$

- 根据更新之后的价值网格参数来更新策略网格参数$\theta\leftarrow:\theta+\alpha\sum_\tau \delta_t\nabla_{\theta}{log(\pi_\theta(a_t|s_t))}$




<center>
<img src="/pics/sk_gl.jpg" width="40%">
</center>