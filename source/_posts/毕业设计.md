---
title: 毕业设计
mathjax: true
categories: 学术
tags:
  - 大模型压缩
  - 毕业设计
abbrlink: 4e025b73
date: 2024-10-27 01:55:46
---


## 经过和导师的沟通之后，最终选定`大模型压缩`作为我的毕业设计主题

[实验室学长提供的思路和文献](https://qcnhulpwswat.feishu.cn/wiki/D2ZAwuFT4iYEQgk4iq6c1xkun0b)

基础常用的量化、剪枝、知识蒸馏等技术的原理本文不再赘述...

### 文献阅读阶段

#### dejavu 基于上下文稀疏性的动态剪枝

>【2024.10.27】`dejavu` 基于MHA、mlp 上下文稀疏性(`contextual sparsity`)的动态预测剪枝

[文献地址](https://proceedings.mlr.press/v202/liu23am.html)

##### 简介

何为所谓`上下文稀疏性`？根据文章作者的想法，大语言模型虽然具有很深很深的层次结构和参数量，但是对于每一部分特定的输入发挥作用/被激活的却只有极少比例的参数（主要分为MHA类和MLP类两类验证）。这其实是相当符合直觉的假设，每次人脑做推理时对于给定的输入，大脑也只有少量神经元（突触）会被激活，而不是一思考整个大脑都被激活（什doge、


<center>
<img src="/pics/context-sparsity.png">
</center>

从上图中不难发现，模型中的上下文稀疏性相当显著。原文中的说法是：small、 input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues.

并且经过作者们实验验证，这样的sparsity can be accurately predicted.
作者设计另一种叫做dejavu的系统对于每一层来动态的根据输入来预测哪些参数可以被剪切。同时实现了在硬件上的并行化处理。[代码地址](https://github.com/FMInference/DejaVu.git)


##### 意义：

之前提出的模型压缩方法总是存在缺陷：（1）基于传统的剪枝方法难以在模型大小和保持效果上达到一个较好的权衡（2）之前的模型压缩方法一般是`task-depandet`，也就是说只能在某些特定任务下达到比较好的效果（3）有研究表明：`lottery ticket hypothesis`[（地址）](https://arxiv.org/abs/1803.03635)表明 iterative pruning方法一般只适用于规模较小的模型。（4）非结构化稀疏性在加速上存在硬件困难[well-known difficulty with modern hardware;Hooker,2021](https://export.arxiv.org/pdf/2009.06489v2.pdf)


##### 问题建模：

- MLP层的稀疏化：

考虑一个`MLP`的连续的两个线性层$W_1 、 W_2 \in R^{d\times 4d}$，输入的向量为$y\in R^{1\times d}$。

那么原始的MLP层的输出为：$y_{out}=\sigma(yW_1){(W_2)}^{T}$;

现在为了充分地利用上上下文稀疏性质，我们就决定对中间一次$4d\times 4d$的矩阵乘法进行稀疏化处理，我们决定根据输入预测哪些$W_1^i、W_2^i \in W_1 、 W_2$同时$\in R^{d\times 1}$是应该被激活的、而其他神经元连接是应该被舍弃的。我们将这些被选中的并入集合$S_M$，稀疏化之后的MLP层变成了：

$$
y_{out}^{S_M} = \sigma(yW_1^{S_M}){(W_2^{S_M})}^{T}
$$

其中$\sigma$是ReLU等激活函数。

- MHA的稀疏化：

根据原文中的说法：假设$X\in R^{n\times d}$是用来进行tokens embedding的矩阵，每个MHA的输入为$y\in R^{1\times d}$，假设这里有h个注意力头，每一个注意力头的QKV三个矩阵描述为$W_i^{K}、W_i^{Q}、W_i^{V}\in R^{d\times d_h}$，最后的合并矩阵为$W_i^O\in R^{d_h\times d}$，我们将每次被激活的注意力头并入集合$S_A$.那么我们的稀疏化注意力机制可以描述为（其中$D_i(y)$是调节常数）:

$$
\begin{align}
MHA_{S_A}(y) & = & \Sigma_{i\in S_A}H_i(y)\times W_i^O\\
H_i(y)&=&{D_i(y)}^{-1}exp(y(W_i^Q)(W_i^K)^TX^T)XW_i^{O}\\
D_i(y)&=&exp(y(W_i^Q)(W_i^K)^TX^T)\times 1^{n} 
\end{align}
$$


![](https://img2022.cnblogs.com/blog/1603920/202209/1603920-20220929093551747-1906322688.png)

<center>
多头注意力机制结构图示
</center>


> 可见问题的关键都在于如何高效的寻找$\quad S_A$、$S_M$

##### 实现预测

为了在预测的时候无需加载MLP、MHA层的高维的参数；论文中决定对于每一层MLP、MHA层都训练一个较小的双全连接层模型来预测哪些参数是应该被激活的。